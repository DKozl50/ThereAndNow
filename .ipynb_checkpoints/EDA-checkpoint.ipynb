{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import sklearn\n",
    "import scipy.sparse\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './readonly/'\n",
    "RESULTS_FOLDER = './results/'\n",
    "\n",
    "data = pd.read_excel(os.path.join(DATA_FOLDER, 'MFTRAVEL_HACKATHON.xlsx'), sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Data', 'CountryDict', 'FieldsDiscr'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = data['CountryDict']['CNTRY_CODE'].values.astype('int32')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxFlightPrice = 100000\n",
    "MaxHotelPrice = 100000\n",
    "MaxHotelPrice = 100000\n",
    "MaxRandDays = 100\n",
    "MaxHotels = 500\n",
    "MaxFlights = 100000\n",
    "\n",
    "ZeroDate = pd.to_datetime('8/2/2020')\n",
    "\n",
    "class Environment:\n",
    "    def gen_dataset(self):\n",
    "        k = dict()\n",
    "        hotels = ['Baba', 'Bubu', 'Kata']\n",
    "        hotel_ind = 0\n",
    "        start_date = ZeroDate\n",
    "\n",
    "        print('Generating country_to_hotels dataset...')\n",
    "        \n",
    "        for i in tqdm_notebook(countries):\n",
    "            country_to_hotels = {'name' : [], 'day_price': [], 'available_from' : [], 'available_to' : []}\n",
    "\n",
    "            for _ in range(MaxHotels):\n",
    "                country_to_hotels['name'].append('{}_{}'.format(hotels[hotel_ind % 3], hotel_ind))\n",
    "                country_to_hotels['day_price'].append(np.random.randint(MaxHotelPrice))\n",
    "\n",
    "                delta1 = np.random.randint(MaxRandDays)\n",
    "                delta2 = np.random.randint(MaxRandDays//2)\n",
    "                country_to_hotels['available_from'].append(start_date + pd.DateOffset(days = delta1))\n",
    "                country_to_hotels['available_to'].append(start_date + pd.DateOffset(days = delta1 + delta2))\n",
    "\n",
    "                hotel_ind+=1\n",
    "\n",
    "            k[i] = pd.DataFrame(data=country_to_hotels)\n",
    "            \n",
    "        self.country_to_hotels_ = k\n",
    "        \n",
    "    def __init__(self, country_to_hotels = None):\n",
    "        print('Generating time_to_planes dataset...')\n",
    "        \n",
    "        d = {'time' : [], 'price': [], 'from': [], 'to' : []}\n",
    "        start_date = ZeroDate\n",
    "        \n",
    "        for i in range(MaxFlights):\n",
    "            if i % (MaxFlights // (MaxRandDays * 2)) == 0: \n",
    "                start_date += pd.DateOffset(days = 1)\n",
    "                \n",
    "            d['time'].append(start_date)\n",
    "            d['price'].append(np.random.randint(MaxFlightPrice))\n",
    "            d['from'].append(countries[np.random.randint(countries.size)])\n",
    "            d['to'].append(countries[np.random.randint(countries.size)])\n",
    "\n",
    "        self.planes_timetable_ = pd.DataFrame(data=d).set_index('time')\n",
    "    \n",
    "        if (country_to_hotels == None):\n",
    "            self.gen_dataset()\n",
    "        else:\n",
    "            self.country_to_hotels_ = country_to_hotels\n",
    "        \n",
    "        \n",
    "        print('Done')\n",
    "        \n",
    "    def get_tickets(self, time1, time2):\n",
    "        return self.planes_timetable_[time1 : time2]\n",
    "    \n",
    "    def get_hotels(self, country_id, time1, time2): #datetime!!!\n",
    "        return self.country_to_hotels_[country_id][\n",
    "            (time1 > self.country_to_hotels_[country_id]['available_from']) &\n",
    "            (time2 < self.country_to_hotels_[country_id]['available_to'])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = env.country_to_hotels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating time_to_planes dataset...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "env = Environment(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>day_price</th>\n",
       "      <th>available_from</th>\n",
       "      <th>available_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Kata_503</td>\n",
       "      <td>57931</td>\n",
       "      <td>2020-08-08</td>\n",
       "      <td>2020-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Kata_506</td>\n",
       "      <td>84984</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>2020-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Baba_510</td>\n",
       "      <td>88833</td>\n",
       "      <td>2020-08-27</td>\n",
       "      <td>2020-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Bubu_523</td>\n",
       "      <td>76752</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>2020-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>Baba_543</td>\n",
       "      <td>27161</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>2020-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>Kata_974</td>\n",
       "      <td>26532</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>2020-09-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>Baba_975</td>\n",
       "      <td>41564</td>\n",
       "      <td>2020-08-24</td>\n",
       "      <td>2020-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>Baba_978</td>\n",
       "      <td>9088</td>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>2020-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>Kata_986</td>\n",
       "      <td>13208</td>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>2020-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>Kata_989</td>\n",
       "      <td>5253</td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>2020-09-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  day_price available_from available_to\n",
       "3    Kata_503      57931     2020-08-08   2020-09-26\n",
       "6    Kata_506      84984     2020-08-06   2020-08-31\n",
       "10   Baba_510      88833     2020-08-27   2020-09-19\n",
       "23   Bubu_523      76752     2020-08-25   2020-09-06\n",
       "43   Baba_543      27161     2020-08-25   2020-09-12\n",
       "..        ...        ...            ...          ...\n",
       "474  Kata_974      26532     2020-08-05   2020-09-14\n",
       "475  Baba_975      41564     2020-08-24   2020-09-09\n",
       "478  Baba_978       9088     2020-08-15   2020-09-07\n",
       "486  Kata_986      13208     2020-08-15   2020-09-27\n",
       "489  Kata_989       5253     2020-08-11   2020-09-19\n",
       "\n",
       "[98 rows x 4 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time1 = pd.to_datetime('28/8/2020')\n",
    "time2 = pd.to_datetime('29/8/2020')\n",
    "\n",
    "env.get_hotels(4, time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>1519</td>\n",
       "      <td>554</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>47774</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>18892</td>\n",
       "      <td>520</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>77592</td>\n",
       "      <td>232</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>56376</td>\n",
       "      <td>344</td>\n",
       "      <td>-2147483648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>24364</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>7228</td>\n",
       "      <td>694</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>73598</td>\n",
       "      <td>4</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>43070</td>\n",
       "      <td>238</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>83443</td>\n",
       "      <td>580</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            price        from          to\n",
       "time                                     \n",
       "2020-08-28   1519         554         208\n",
       "2020-08-28  47774         104         104\n",
       "2020-08-28  18892         520         404\n",
       "2020-08-28  77592         232          64\n",
       "2020-08-28  56376         344 -2147483648\n",
       "...           ...         ...         ...\n",
       "2020-08-29  24364 -2147483648         876\n",
       "2020-08-29   7228         694         860\n",
       "2020-08-29  73598           4         694\n",
       "2020-08-29  43070         238         124\n",
       "2020-08-29  83443         580         136\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_tickets(time1, time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = data['Data'].copy()\n",
    "\n",
    "def foo(x):\n",
    "    if type(x) == int:\n",
    "        return [x]\n",
    "    return list(map(int, x.split('; ')))\n",
    "\n",
    "exp['COUNTRIES_IN_TRIP'] = exp['COUNTRIES_IN_TRIP'].apply(foo)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKAY GENDER\n",
      "OKAY REGION\n",
      "OKAY DEVICE_TYPE\n",
      "OKAY OS\n",
      "OKAY SUBSAGE_MF_SEGMENT\n"
     ]
    }
   ],
   "source": [
    "exp['USING_INTERNET'] = exp['USING_INTERNET'].fillna(0).astype('bool')\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in ['GENDER', 'REGION', 'DEVICE_TYPE',\n",
    "               'OS', 'SUBSAGE_MF_SEGMENT']:\n",
    "    le.fit(list(exp[column].values))\n",
    "    exp[column] = le.transform(list(exp[column].values))\n",
    "    print('OKAY', column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT\"S HEAVY\n",
    "visited_country = dict()\n",
    "for i in countries:\n",
    "    list_t = []\n",
    "    for j in exp['COUNTRIES_IN_TRIP']:\n",
    "        list_t.append(i in j)\n",
    "    \n",
    "    visited_country[i] = list_t\n",
    "    \n",
    "exp = exp.join(pd.DataFrame(visited_country))\n",
    "exp.drop(['COUNTRIES_IN_TRIP'], inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp['User_ID'] = exp['User_ID'].astype('int32')\n",
    "exp['AGE'] = exp['AGE'].fillna(-1).astype('int8')\n",
    "exp['GENDER'] = exp['GENDER'].astype('int8')\n",
    "exp['REGION'] = exp['REGION'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo = exp.groupby(['User_ID']).agg({'SMS_IN_CNT_M3' : ['mean', 'std'], \n",
    "                                      'TRIP_DURATION' : ['mean', 'std'],\n",
    "                                      'MOU_IN_REVENUE_M3' : ['mean', 'std'],\n",
    "                                      'MOU_OUT_REVENUE_M3' : ['mean', 'std'], \n",
    "                                      'DOU_DURATION_M3' : ['mean', 'std'], \n",
    "                                      'ARPU_M3' : ['mean', 'std']\n",
    "                                     })\n",
    "foo = lambda x : \"_\".join(x) if x[1]!=\"\" else x[0]\n",
    "tempo.columns = [foo(x) for x in tempo.columns.ravel()]\n",
    "\n",
    "user_to_flights = exp.groupby(['User_ID'])[exp.columns[18:]].sum().values\n",
    "#user_to_mostfreq_country = countries[user_to_flights.argsort(axis=1)[-3:][::-1]]\n",
    "user_to_mostfreq_country = countries[user_to_flights.argmax(axis=1)]\n",
    "\n",
    "tempo['most_freq'] = user_to_mostfreq_country\n",
    "user_data = tempo.join(exp.groupby(['User_ID']).agg({'GENDER' : 'max', 'AGE' : 'max', 'REGION' : 'max',\n",
    "                              'DEVICE_TYPE' : 'max', 'OS' : 'max', 'SUBSAGE_MF_SEGMENT' : 'max', \n",
    "                              'USING_INTERNET' : 'max'}))\n",
    "del tempo\n",
    "user_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>REGION</th>\n",
       "      <th>DEVICE_TYPE</th>\n",
       "      <th>OS</th>\n",
       "      <th>SUBSAGE_MF_SEGMENT</th>\n",
       "      <th>USING_INTERNET</th>\n",
       "      <th>TRIP_DURATION</th>\n",
       "      <th>TRIP_MAIN_COUNTRY</th>\n",
       "      <th>...</th>\n",
       "      <th>732</th>\n",
       "      <th>887</th>\n",
       "      <th>894</th>\n",
       "      <th>716</th>\n",
       "      <th>start_month</th>\n",
       "      <th>start_day</th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_month</th>\n",
       "      <th>end_day</th>\n",
       "      <th>end_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>80000</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>222</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>80000</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>80001</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>213</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>80001</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>80001</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378462</td>\n",
       "      <td>123215</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378463</td>\n",
       "      <td>177003</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378464</td>\n",
       "      <td>180451</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378465</td>\n",
       "      <td>156886</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378466</td>\n",
       "      <td>180452</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378467 rows × 267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_ID  GENDER  AGE  REGION  DEVICE_TYPE  OS  SUBSAGE_MF_SEGMENT  \\\n",
       "0         80000       2   48      55            5   0                   1   \n",
       "1         80000       2   48      55            5   0                   1   \n",
       "2         80001       0   -1      55            5   0                   1   \n",
       "3         80001       0   -1      55            5   0                   1   \n",
       "4         80001       0   -1      55            5   0                   1   \n",
       "...         ...     ...  ...     ...          ...  ..                 ...   \n",
       "378462   123215       0   -1       0            9  21                   0   \n",
       "378463   177003       0   -1       0            9  21                   0   \n",
       "378464   180451       0   -1       0            9  21                   0   \n",
       "378465   156886       0   -1      33            5  20                   2   \n",
       "378466   180452       0   -1      35            5  20                   2   \n",
       "\n",
       "        USING_INTERNET  TRIP_DURATION  TRIP_MAIN_COUNTRY  ...    732    887  \\\n",
       "0                 True            5.0                222  ...  False  False   \n",
       "1                 True            6.0                 85  ...  False  False   \n",
       "2                 True            2.0                213  ...  False  False   \n",
       "3                 True            3.0                 23  ...  False  False   \n",
       "4                 True            5.0                202  ...  False  False   \n",
       "...                ...            ...                ...  ...    ...    ...   \n",
       "378462           False            5.0                 19  ...  False  False   \n",
       "378463           False            1.0                175  ...  False  False   \n",
       "378464           False           13.0                  1  ...  False  False   \n",
       "378465            True           12.0                 12  ...  False  False   \n",
       "378466            True            NaN                189  ...  False  False   \n",
       "\n",
       "          894    716  start_month  start_day  start_year  end_month  end_day  \\\n",
       "0       False  False           10          3        2019         10        7   \n",
       "1       False  False           10         10        2019         10       15   \n",
       "2       False  False           12          1        2019         12        2   \n",
       "3       False  False           12         18        2019         12       20   \n",
       "4       False  False           12         27        2019         12       31   \n",
       "...       ...    ...          ...        ...         ...        ...      ...   \n",
       "378462  False  False            8         11        2019          8       15   \n",
       "378463  False  False            3          8        2019          3        8   \n",
       "378464  False  False            7         13        2019          7       25   \n",
       "378465  False  False            2          8        2019          2       19   \n",
       "378466  False  False            3         17        2019          3       21   \n",
       "\n",
       "        end_year  \n",
       "0           2019  \n",
       "1           2019  \n",
       "2           2019  \n",
       "3           2019  \n",
       "4           2019  \n",
       "...          ...  \n",
       "378462      2019  \n",
       "378463      2019  \n",
       "378464      2019  \n",
       "378465      2019  \n",
       "378466      2019  \n",
       "\n",
       "[378467 rows x 267 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp['start_month'] = exp['START_TRIP'].apply(lambda x : x.month)\n",
    "exp['start_day'] = exp['START_TRIP'].apply(lambda x : x.day)\n",
    "exp['start_year'] = exp['START_TRIP'].apply(lambda x : x.year)\n",
    "\n",
    "exp['end_month'] = exp['END_TRIP'].apply(lambda x : x.month)\n",
    "exp['end_day'] = exp['END_TRIP'].apply(lambda x : x.day)\n",
    "exp['end_year'] = exp['END_TRIP'].apply(lambda x : x.year)\n",
    "\n",
    "exp.drop(['END_TRIP', 'START_TRIP'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Reshape, Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras.layers import LeakyReLU, Dropout, ReLU, BatchNormalization, InputLayer, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "\n",
    "inp = tf.keras.layers.Input((22,))\n",
    "\n",
    "k = Dense(32)(inp)\n",
    "k = LeakyReLU(alpha=0.2)(k)\n",
    "k = Dropout(dropout)(k)\n",
    "\n",
    "k = Dense(64)(k)\n",
    "k = LeakyReLU(alpha=0.2)(k)\n",
    "k = Dropout(dropout)(k)\n",
    "\n",
    "k = Dense(64)(k)\n",
    "k = LeakyReLU(alpha=0.2)(k)\n",
    "k = Dropout(dropout)(k)\n",
    "\n",
    "k = Dense(32)(k)\n",
    "k = LeakyReLU(alpha=0.2)(k)\n",
    "k = Dropout(dropout)(k)\n",
    "\n",
    "k = Dense(16)(k)\n",
    "k = LeakyReLU(alpha=0.2)(k)\n",
    "k = Dropout(dropout)(k)\n",
    "\n",
    "k = Dense(1)(k)\n",
    "\n",
    "D = tf.keras.Model(\n",
    "            inputs=inp, \n",
    "            outputs=Activation('sigmoid')(k))\n",
    "\n",
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "D.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "\n",
    "inputs_ = tf.keras.layers.Input((21,))\n",
    "\n",
    "batch_norm = BatchNormalization()(inputs_)\n",
    "\n",
    "dense_1 = Dense(32)(batch_norm)\n",
    "nonlin_1 = LeakyReLU(alpha=0.2)(dense_1)\n",
    "drop_1 = Dropout(dropout)(nonlin_1)\n",
    "\n",
    "dense_2 = Dense(64)(drop_1)\n",
    "nonlin_2 = LeakyReLU(alpha=0.2)(dense_2)\n",
    "drop_2 = Dropout(dropout)(nonlin_2)\n",
    "\n",
    "dense_3 = Dense(32)(drop_2)\n",
    "nonlin_3 = LeakyReLU(alpha=0.2)(dense_3)\n",
    "drop_3 = Dropout(dropout)(nonlin_3)\n",
    "\n",
    "predictions = ReLU()(Dense(1)(drop_3))\n",
    "\n",
    "conc = tf.concat(axis=1,values=[inputs_, predictions])\n",
    "\n",
    "optimizer = Adam(1e-4)\n",
    "G = tf.keras.Model(\n",
    "            inputs=inputs_, \n",
    "            outputs=conc)\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return tf.reduce_mean((y_pred[:, -1:]-y[:, -1:])**2)\n",
    "\n",
    "G.compile(loss=loss, optimizer=optimizer)\n",
    "#conc = Concatenate(axis=1)([inputs_, predictions.output])\n",
    "\n",
    "res_D = D(conc)\n",
    "\n",
    "AM =  tf.keras.Model(\n",
    "            inputs=inputs_, \n",
    "            outputs=res_D)\n",
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []\n",
    "for i in ['start', 'end']:\n",
    "    for j in ['day', 'month', 'year']:\n",
    "        time_list.append('{}_{}'.format(i, j))\n",
    "        \n",
    "ids_list = user_data.index.unique()\n",
    "\n",
    "epochs = 50\n",
    "batch_size_noise = 64\n",
    "batch_size_true = 128\n",
    "\n",
    "#true_data = exp[['User_ID', 'TRIP_MAIN_COUNTRY'] + time_list].join(user_data, on = 'User_ID')\n",
    "true_data = exp[['User_ID', 'TRIP_MAIN_COUNTRY']].join(user_data, on = 'User_ID')\n",
    "\n",
    "cols = true_data.columns.tolist()\n",
    "true_data = true_data[cols[0:1] + cols[2:] + cols[1:2] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_orig = dict()\n",
    "#for i in time_list: \n",
    "#    d_orig[i] = true_data[i].max()\n",
    "#    true_data[i] /= true_data[i].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20985.822 - 0s 172us/sample - loss: 48674.9131\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 35608.156 - 0s 172us/sample - loss: 25333.1885\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22200.591 - 0s 180us/sample - loss: 21283.8276\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34292612.00 - 0s 156us/sample - loss: 8591427.5791\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20018.882 - 0s 180us/sample - loss: 20243.5664\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17693.830 - 0s 172us/sample - loss: 22245.9429\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15073.516 - 0s 180us/sample - loss: 47397.4021\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20682.332 - 0s 188us/sample - loss: 1541419.8823\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24381.293 - 0s 141us/sample - loss: 23280.0337\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20026.330 - 0s 180us/sample - loss: 22426.9067\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20498.839 - 0s 180us/sample - loss: 22264.9761\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16512.601 - 0s 187us/sample - loss: 18850.8882\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22659.746 - 0s 156us/sample - loss: 24454.2827\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21516.722 - 0s 148us/sample - loss: 24767.6626\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22240.730 - 0s 188us/sample - loss: 22016.8223\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22416.462 - 0s 187us/sample - loss: 12684374.9668\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 25095.484 - 0s 187us/sample - loss: 8645165.0186\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 26121.757 - 0s 172us/sample - loss: 48817.3818\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20905.291 - 0s 195us/sample - loss: 21468.4565\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 25167.269 - 0s 211us/sample - loss: 24184.8462\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24686.363 - 0s 164us/sample - loss: 21673.3716\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19056.927 - 0s 156us/sample - loss: 21674.3203\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21798.554 - 0s 172us/sample - loss: 23688.3149\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17770.589 - 0s 195us/sample - loss: 44791.7935\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19582.457 - 0s 172us/sample - loss: 48099.3350\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 33399.796 - 0s 188us/sample - loss: 22706.9517\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12273.726 - 0s 164us/sample - loss: 53904.3564\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20677.933 - 0s 148us/sample - loss: 22401.4712\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 32068.582 - 0s 164us/sample - loss: 24992.6260\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21808.048 - 0s 164us/sample - loss: 20877.6650\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20909.345 - 0s 172us/sample - loss: 1154045.3306\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20848.390 - 0s 156us/sample - loss: 1853760.9355\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22526.160 - 0s 164us/sample - loss: 21463.9541\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21521.816 - 0s 172us/sample - loss: 22747.9668\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 126755.09 - 0s 164us/sample - loss: 49280.2837\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23378.902 - 0s 148us/sample - loss: 21469.9194\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22821.927 - 0s 187us/sample - loss: 22862.3545\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19883.777 - 0s 180us/sample - loss: 20152.9041\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18448.871 - 0s 180us/sample - loss: 23291.9517\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22891.843 - 0s 156us/sample - loss: 21309.6973\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20541.480 - 0s 164us/sample - loss: 19197.2136\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18423.240 - 0s 172us/sample - loss: 20128.5034\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24410.806 - 0s 172us/sample - loss: 21888.9377\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23398.062 - 0s 180us/sample - loss: 22920.6343\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23425.337 - 0s 172us/sample - loss: 21518.6909\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23235.359 - 0s 164us/sample - loss: 20770.3235\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22953.382 - 0s 172us/sample - loss: 826011.4302\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19073.968 - 0s 164us/sample - loss: 19738.6133\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23948.039 - 0s 203us/sample - loss: 48785.8647\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20438.746 - 0s 156us/sample - loss: 19635.1909\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 50679944.00 - 0s 180us/sample - loss: 12686677.5884\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20200.730 - 0s 164us/sample - loss: 20484.3179\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22854.707 - 0s 180us/sample - loss: 21996.7212\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19183.294 - 0s 172us/sample - loss: 21795.9688\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20098.562 - 0s 180us/sample - loss: 23854.4946\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21879.123 - 0s 188us/sample - loss: 20417.1748\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20087.007 - 0s 172us/sample - loss: 45371.9893\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15386.585 - 0s 172us/sample - loss: 74089.9766\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15054.085 - 0s 156us/sample - loss: 8587761.4668\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24117.718 - 0s 164us/sample - loss: 22723.6582\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 29901.621 - 0s 164us/sample - loss: 21503.9443\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19369.750 - 0s 172us/sample - loss: 22053.1875\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18979.695 - 0s 172us/sample - loss: 19601.5649\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22081.773 - 0s 164us/sample - loss: 13237259.4556\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 37372.906 - 0s 180us/sample - loss: 24304.9458\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 29950.773 - 0s 164us/sample - loss: 24026.9382\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14097.245 - 0s 164us/sample - loss: 19213.9597\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18398.753 - 0s 156us/sample - loss: 8633177.3613\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17262.326 - 0s 164us/sample - loss: 21111.8574\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18543.828 - 0s 164us/sample - loss: 1535847.3413\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18973.576 - 0s 172us/sample - loss: 18659.2319\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23573.554 - 0s 172us/sample - loss: 49122.7202\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20182.425 - 0s 156us/sample - loss: 20591.9673\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16997.117 - 0s 164us/sample - loss: 20279.0791\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17667.962 - 0s 172us/sample - loss: 21749.7651\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34323460.00 - 0s 414us/sample - loss: 8597007.1909\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21679.863 - 0s 180us/sample - loss: 19354.4719\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17092.826 - 0s 172us/sample - loss: 24220.5532\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22619.878 - 0s 148us/sample - loss: 19960.0278\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14450.315 - 0s 148us/sample - loss: 20651.7351\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16112.295 - 0s 180us/sample - loss: 22186.8870\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 128586.88 - 0s 172us/sample - loss: 45750.6040\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22102.347 - 0s 172us/sample - loss: 36020.2124\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 126751.85 - 0s 164us/sample - loss: 49687.3125\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18956.642 - 0s 164us/sample - loss: 18093.7036\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22101.214 - 0s 148us/sample - loss: 44444.4971\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24474.082 - 0s 180us/sample - loss: 22678.4854\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20084.806 - 0s 180us/sample - loss: 20870.0576\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20067.726 - 0s 195us/sample - loss: 19781.4746\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24171.310 - 0s 164us/sample - loss: 12680700.4365\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 473260.59 - 0s 148us/sample - loss: 132684.7458\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21547.160 - 0s 172us/sample - loss: 25631.3564\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20606.847 - 0s 164us/sample - loss: 20690.6206\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 25127.011 - 0s 172us/sample - loss: 8678775.9404\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21784.609 - 0s 172us/sample - loss: 22624.0747\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21060.093 - 0s 180us/sample - loss: 16926.2888\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19141.410 - 0s 180us/sample - loss: 22729.4189\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17066.673 - 0s 164us/sample - loss: 20414.7402\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16522.302 - 0s 180us/sample - loss: 19874.5103\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18712.886 - 0s 188us/sample - loss: 18011.6753\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22263.408 - 0s 172us/sample - loss: 45832.7554\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19420.746 - 0s 180us/sample - loss: 47336.4307\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15553.543 - 0s 180us/sample - loss: 17227.9539\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22946.765 - 0s 172us/sample - loss: 21310.5210\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14786.615 - 0s 164us/sample - loss: 19047.2017\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16296.472 - 0s 156us/sample - loss: 19169.0803\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23091.609 - 0s 141us/sample - loss: 20966.9126\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23083.238 - 0s 156us/sample - loss: 18738.4822\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19497.203 - 0s 172us/sample - loss: 20518.9209\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20149.994 - 0s 172us/sample - loss: 22673.3682\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24038.949 - 0s 172us/sample - loss: 20274.4185\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23602.152 - 0s 164us/sample - loss: 18922.3394\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22763.244 - 0s 172us/sample - loss: 8614109.6255\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18203.726 - 0s 172us/sample - loss: 45799.8076\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15602.961 - 0s 180us/sample - loss: 18401.3777\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22128.585 - 0s 156us/sample - loss: 47501.6099\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 24562.752 - 0s 172us/sample - loss: 17730.7024\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20166.601 - 0s 164us/sample - loss: 18955.1060\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17401.136 - 0s 156us/sample - loss: 19553.1636\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20819.632 - 0s 156us/sample - loss: 18507.6904\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34388060.00 - 0s 164us/sample - loss: 8613541.6367\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 125542.64 - 0s 164us/sample - loss: 47207.4912\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21028.867 - 0s 156us/sample - loss: 22153.4072\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17819.584 - 0s 172us/sample - loss: 19330.9211\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20471.871 - 0s 164us/sample - loss: 17619.1675\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19541.562 - 0s 156us/sample - loss: 1534115.3115\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 125927.25 - 0s 180us/sample - loss: 1560281.2881\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22215.896 - 0s 164us/sample - loss: 19121.0439\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 17339.705 - 0s 164us/sample - loss: 21967.5537\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17026.964 - 0s 172us/sample - loss: 43468.8936\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34393660.00 - 0s 164us/sample - loss: 8612796.5444\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20995.320 - 0s 172us/sample - loss: 12675118.7217\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 192983.62 - 0s 180us/sample - loss: 64324.1660\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19053.726 - 0s 156us/sample - loss: 20212.9209\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12802.330 - 0s 164us/sample - loss: 19673.1929\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 26493.867 - 0s 172us/sample - loss: 8624633.5156\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21043.031 - 0s 172us/sample - loss: 19573.3369\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19744.568 - 0s 172us/sample - loss: 21064.7192\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17396.433 - 0s 172us/sample - loss: 18197.2202\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18368.281 - 0s 180us/sample - loss: 18661.8501\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19138.814 - 0s 172us/sample - loss: 16699.0564\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17818.550 - 0s 164us/sample - loss: 19390.4258\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17127.199 - 0s 164us/sample - loss: 18359.0962\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16060.847 - 0s 156us/sample - loss: 19319.0469\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 23372.550 - 0s 172us/sample - loss: 8614018.2910\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 52604752.00 - 0s 187us/sample - loss: 13189993.7373\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16699.826 - 0s 156us/sample - loss: 19024.2393\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22451.369 - 0s 179us/sample - loss: 24110.2349\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18230.402 - 0s 172us/sample - loss: 17627.5496\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13355.699 - 0s 156us/sample - loss: 17653.2397\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19003.492 - 0s 180us/sample - loss: 70156.6619\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13697.716 - 0s 180us/sample - loss: 19925.4443\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16183.044 - 0s 172us/sample - loss: 16129.5310\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19517.488 - 0s 164us/sample - loss: 8625210.9199\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19351.515 - 0s 187us/sample - loss: 43136.1113\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15675.268 - 0s 180us/sample - loss: 16990.5154\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21479.308 - 0s 172us/sample - loss: 19640.8489\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 118985.14 - 0s 172us/sample - loss: 42777.7065\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16620.927 - 0s 172us/sample - loss: 17899.3872\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15978.507 - 0s 188us/sample - loss: 18103.3171\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16531.503 - 0s 180us/sample - loss: 17192.7612\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 22227.679 - 0s 172us/sample - loss: 17787.6670\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19504.160 - 0s 180us/sample - loss: 46061.4351\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15114.894 - 0s 164us/sample - loss: 16057.4463\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19105.685 - 0s 164us/sample - loss: 19122.6826\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18132.812 - 0s 164us/sample - loss: 68042.0347\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16386.543 - 0s 164us/sample - loss: 17164.1663\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14326.572 - 0s 156us/sample - loss: 17104.8633\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21969.433 - 0s 164us/sample - loss: 12667640.9648\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13787.793 - 0s 187us/sample - loss: 16879.1086\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 197617.60 - 0s 164us/sample - loss: 61991.3125\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34411336.00 - 0s 187us/sample - loss: 8613367.1135\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 5524121.500 - 0s 180us/sample - loss: 1395737.2080\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14779.507 - 0s 172us/sample - loss: 20110.4517\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13206.627 - 0s 156us/sample - loss: 15685.0120\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21327.884 - 0s 172us/sample - loss: 19286.8447\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15386.227 - 0s 172us/sample - loss: 16702.7888\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19678.757 - 0s 172us/sample - loss: 19188.8975\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20262.736 - 0s 188us/sample - loss: 17355.2156\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14975.712 - 0s 172us/sample - loss: 14643.9241\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17689.918 - 0s 164us/sample - loss: 18731.0420\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15643.526 - 0s 180us/sample - loss: 17107.1624\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20730.753 - 0s 164us/sample - loss: 16654.6455\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11348.499 - 0s 180us/sample - loss: 17158.7854\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13761.481 - 0s 172us/sample - loss: 16711.6282\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16510.113 - 0s 172us/sample - loss: 17391.6458\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34230140.00 - 0s 172us/sample - loss: 8570867.5911\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16914.593 - 0s 156us/sample - loss: 43249.6094\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19055.519 - 0s 164us/sample - loss: 17513.8411\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13302.556 - 0s 164us/sample - loss: 8559306.3062\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14637.464 - 0s 164us/sample - loss: 17144.2490\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16806.730 - 0s 164us/sample - loss: 15663.7642\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 15957.855 - 0s 164us/sample - loss: 40913.6147\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18400.921 - 0s 156us/sample - loss: 19831.3198\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14445.554 - 0s 148us/sample - loss: 15350.8872\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16848.955 - 0s 164us/sample - loss: 19488.8735\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17115.072 - 0s 180us/sample - loss: 43519.6680\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17136.515 - 0s 172us/sample - loss: 18548.7241\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14069.853 - 0s 164us/sample - loss: 15087.7107\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 112400.19 - 0s 164us/sample - loss: 66474.2615\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20307.093 - 0s 172us/sample - loss: 18408.5232\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13046.641 - 0s 187us/sample - loss: 14196.3062\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16634.103 - 0s 148us/sample - loss: 14960.7864\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 21586.242 - 0s 164us/sample - loss: 18576.2563\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 25629.701 - 0s 172us/sample - loss: 20145.1694\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14540.370 - 0s 164us/sample - loss: 16012.0745\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12705.666 - 0s 211us/sample - loss: 14613.1555\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17428.363 - 0s 180us/sample - loss: 16219.8608\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15451.454 - 0s 180us/sample - loss: 18473.8625\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14241.904 - 0s 172us/sample - loss: 1525659.7920\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17824.835 - 0s 203us/sample - loss: 15537.2703\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10216.533 - 0s 195us/sample - loss: 8591860.3091\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13193.341 - 0s 187us/sample - loss: 14909.1030\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34301224.00 - 0s 195us/sample - loss: 17204805.1494\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18593.062 - 0s 156us/sample - loss: 39648.1016\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17497024.00 - 0s 180us/sample - loss: 4385733.5137\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16639.343 - 0s 203us/sample - loss: 14487.1821\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 122898.36 - 0s 172us/sample - loss: 42312.6262\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12950.800 - 0s 172us/sample - loss: 8578029.2878\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13515.714 - 0s 172us/sample - loss: 17844.7837\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 111576.21 - 0s 164us/sample - loss: 36899.7827\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14975.438 - 0s 180us/sample - loss: 13393.9863\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11530.259 - 0s 180us/sample - loss: 19185.8767\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10061.416 - 0s 203us/sample - loss: 13154473.3162\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14769.330 - 0s 172us/sample - loss: 13221.9041\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20995.859 - 0s 195us/sample - loss: 16475.2480\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13008.393 - 0s 172us/sample - loss: 12962.2512\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18031.207 - 0s 172us/sample - loss: 15551.9478\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12004.955 - 0s 172us/sample - loss: 11144.4072\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14375.707 - 0s 188us/sample - loss: 14296.6130\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15796.880 - 0s 172us/sample - loss: 15176.1738\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13795.379 - 0s 172us/sample - loss: 13355.3733\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18770.910 - 0s 172us/sample - loss: 17442.6318\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14728.734 - 0s 172us/sample - loss: 8595008.3123\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12487.998 - 0s 180us/sample - loss: 14506327.7290\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15442.845 - 0s 156us/sample - loss: 15584.2937\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18581.668 - 0s 172us/sample - loss: 14813.9856\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14898.060 - 0s 172us/sample - loss: 13920.6309\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18958.294 - 0s 148us/sample - loss: 15272.0669\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10003.647 - 0s 188us/sample - loss: 11888.4646\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11111.007 - 0s 203us/sample - loss: 14629.9985\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12129.016 - 0s 148us/sample - loss: 11398.2715\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11887.461 - 0s 172us/sample - loss: 38362.0425\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12264.593 - 0s 180us/sample - loss: 13519.5361\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13748.137 - 0s 164us/sample - loss: 14366.5054\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 20380.947 - 0s 180us/sample - loss: 14127.1086\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16870.941 - 0s 172us/sample - loss: 42420.9619\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 25683.746 - 0s 172us/sample - loss: 41335.4478\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8870.20 - 0s 203us/sample - loss: 15489.2610\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7316190.000 - 0s 188us/sample - loss: 12249579.6748\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6065493.500 - 0s 156us/sample - loss: 1526816.0249\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14588.138 - 0s 172us/sample - loss: 14293.5115\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9333.33 - 0s 172us/sample - loss: 36553.1519\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 27218.468 - 0s 188us/sample - loss: 18012.3154\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15391.961 - 0s 180us/sample - loss: 18117.8323\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17016.683 - 0s 180us/sample - loss: 40043.5083\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 12151.418 - 0s 180us/sample - loss: 12782.1428\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14482.892 - 0s 164us/sample - loss: 15161.9561\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14254.105 - 0s 156us/sample - loss: 39967.1904\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14571.251 - 0s 188us/sample - loss: 13875.5481\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13312.228 - 0s 172us/sample - loss: 13338.4736\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10517.174 - 0s 203us/sample - loss: 12554.6921\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 119775.77 - 0s 203us/sample - loss: 38845.2144\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17344.580 - 0s 211us/sample - loss: 13594.1758\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17150.644 - 0s 156us/sample - loss: 40118.1848\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15827.532 - 0s 172us/sample - loss: 37375.7068\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17746.074 - 0s 195us/sample - loss: 14869.6155\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12995.345 - 0s 172us/sample - loss: 11699.0410\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14373.648 - 0s 172us/sample - loss: 13061.5312\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13489.687 - 0s 164us/sample - loss: 11952.2920\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15527.253 - 0s 172us/sample - loss: 15381.1729\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11100.621 - 0s 203us/sample - loss: 14941.7495\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19412.437 - 0s 164us/sample - loss: 13123.6741\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15084.084 - 0s 164us/sample - loss: 1525465.9092\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13954.648 - 0s 180us/sample - loss: 13923.3650\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12048.990 - 0s 187us/sample - loss: 14155.5552\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12602.110 - 0s 172us/sample - loss: 13657.8198\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15864.807 - 0s 164us/sample - loss: 13136995.6340\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18205.996 - 0s 172us/sample - loss: 65560.2100\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16468.330 - 0s 180us/sample - loss: 14965.4121\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15099.664 - 0s 172us/sample - loss: 37470.9976\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10231.303 - 0s 172us/sample - loss: 11862.9194\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9669.84 - 0s 180us/sample - loss: 37545.4448\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14593.968 - 0s 164us/sample - loss: 13598.0967\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9331.36 - 0s 180us/sample - loss: 13167445.7773\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15325.563 - 0s 180us/sample - loss: 13435.6472\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14952.092 - 0s 172us/sample - loss: 12496.8083\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 18042.529 - 0s 164us/sample - loss: 37803.2585\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14999.162 - 0s 172us/sample - loss: 8627593.9802\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10481.480 - 0s 156us/sample - loss: 37571.4397\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10523.822 - 0s 172us/sample - loss: 13232.0659\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12137.209 - 0s 187us/sample - loss: 15979.8772\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11212.053 - 0s 180us/sample - loss: 10066.8423\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11443.592 - 0s 164us/sample - loss: 8599289.2405\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11160.922 - 0s 148us/sample - loss: 12424.8115\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13111.056 - 0s 172us/sample - loss: 36161.7146\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13857.367 - 0s 180us/sample - loss: 13687.6943\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13095.802 - 0s 164us/sample - loss: 11690.4634\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9387.91 - 0s 156us/sample - loss: 1510899.9341\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10006.847 - 0s 156us/sample - loss: 10887.6057\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13565.493 - 0s 156us/sample - loss: 13115.8757\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12763.216 - 0s 172us/sample - loss: 13113.5168\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11207.844 - 0s 180us/sample - loss: 11519.6914\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9186.94 - 0s 164us/sample - loss: 10231.2222\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11494.990 - 0s 172us/sample - loss: 12267.7605\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10476.918 - 0s 180us/sample - loss: 12011.7795\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11064.322 - 0s 187us/sample - loss: 11442.0200\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17931.500 - 0s 172us/sample - loss: 13317.2722\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15730.265 - 0s 180us/sample - loss: 14133.0696\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13019.498 - 0s 172us/sample - loss: 38493.0579\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13878.480 - 0s 172us/sample - loss: 14681.5005\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12033.819 - 0s 180us/sample - loss: 12190.7891\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11963.603 - 0s 164us/sample - loss: 18492.0071\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 123672.06 - 0s 180us/sample - loss: 39406.5488\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11793.104 - 0s 180us/sample - loss: 12279.5164\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12309.810 - 0s 180us/sample - loss: 12575.7300\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14127.873 - 0s 172us/sample - loss: 37104.1943\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13171.720 - 0s 195us/sample - loss: 12740.1248\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9798.22 - 0s 180us/sample - loss: 10819.2942\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11613.611 - 0s 187us/sample - loss: 11046.2482\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 11145.990 - 0s 156us/sample - loss: 11457.4868\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13656.324 - 0s 188us/sample - loss: 12943.1582\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34292436.00 - 0s 164us/sample - loss: 8607407.5771\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9438.25 - 0s 156us/sample - loss: 10151.8320\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10147.245 - 0s 164us/sample - loss: 8595461.3203\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12571.347 - 0s 172us/sample - loss: 12825.6279\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14156.007 - 0s 172us/sample - loss: 12174.5933\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10010.212 - 0s 187us/sample - loss: 10138.2061\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19790.994 - 0s 172us/sample - loss: 14303.8933\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11395.589 - 0s 172us/sample - loss: 38173.4961\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14389.667 - 0s 172us/sample - loss: 11893.2292\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8948.12 - 0s 195us/sample - loss: 1163107.3203\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11573.003 - 0s 156us/sample - loss: 11081.6030\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9980.22 - 0s 164us/sample - loss: 11718.9407\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11091.884 - 0s 156us/sample - loss: 36666.7168\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12128.265 - 0s 172us/sample - loss: 13476.8054\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12870.697 - 0s 164us/sample - loss: 10293.2683\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11095.425 - 0s 164us/sample - loss: 36349.5947\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8809.76 - 0s 156us/sample - loss: 11064.3367\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 212823.68 - 0s 172us/sample - loss: 62829.1487\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12450.904 - 0s 148us/sample - loss: 10831.7173\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 105196.43 - 0s 172us/sample - loss: 37065.9885\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11458.486 - 0s 156us/sample - loss: 10458.6129\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10956.320 - 0s 180us/sample - loss: 10254.7153\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11130.960 - 0s 164us/sample - loss: 8589046.5547\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13444.373 - 0s 172us/sample - loss: 11808.9834\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8352.24 - 0s 156us/sample - loss: 33952.5242\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11661.326 - 0s 172us/sample - loss: 11797.2651\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12485.994 - 0s 164us/sample - loss: 10122.2477\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10688.018 - 0s 164us/sample - loss: 6558965.2966\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11320.349 - 0s 180us/sample - loss: 9378.7155\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19334.214 - 0s 172us/sample - loss: 13745.2415\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8114.64 - 0s 172us/sample - loss: 12662.0309\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10004.763 - 0s 188us/sample - loss: 10213.5273\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8752.52 - 0s 172us/sample - loss: 33688.4518\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10501.998 - 0s 164us/sample - loss: 13999.4463\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9333.95 - 0s 148us/sample - loss: 8596362.8386\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7987.70 - 0s 180us/sample - loss: 9711.2101\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9845.48 - 0s 164us/sample - loss: 8578202.7659\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10799.602 - 0s 180us/sample - loss: 12363.0542\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9541.94 - 0s 172us/sample - loss: 10068.7618\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17390.640 - 0s 188us/sample - loss: 10920.0347\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11603.043 - 0s 172us/sample - loss: 12971.8232\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9917.83 - 0s 180us/sample - loss: 9722.4396\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11493.617 - 0s 164us/sample - loss: 11043.1421\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11496.956 - 0s 172us/sample - loss: 34378.3340\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9955.81 - 0s 180us/sample - loss: 11218.8093\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11791.911 - 0s 156us/sample - loss: 11845.2515\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 204424.54 - 0s 164us/sample - loss: 58348.9458\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8713.28 - 0s 164us/sample - loss: 8878.2026\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12501.587 - 0s 156us/sample - loss: 11812.3853\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12412.263 - 0s 156us/sample - loss: 11207.3132\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9904.03 - 0s 156us/sample - loss: 8731.6215\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34280984.00 - 0s 164us/sample - loss: 8578644.8215\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11332.151 - 0s 164us/sample - loss: 11567.9807\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11081.230 - 0s 188us/sample - loss: 10894.4827\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12762.804 - 0s 172us/sample - loss: 10399.2397\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8976.24 - 0s 156us/sample - loss: 8546924.1951\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9186.67 - 0s 172us/sample - loss: 9131.1333\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10871.607 - 0s 172us/sample - loss: 9999.4597\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7927.51 - 0s 180us/sample - loss: 8896.7139\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7297119.500 - 0s 172us/sample - loss: 1832183.3044\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 52497576.00 - 0s 164us/sample - loss: 13134177.6025\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11317.648 - 0s 164us/sample - loss: 11618.3535\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8121.20 - 0s 164us/sample - loss: 1859558.0046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10840.627 - 0s 172us/sample - loss: 60649.0852\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12302.207 - 0s 172us/sample - loss: 12070.9734\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9987.41 - 0s 195us/sample - loss: 32790.5776\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10448.686 - 0s 156us/sample - loss: 9166.1573\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10448.460 - 0s 180us/sample - loss: 9354.1960\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7338109.000 - 0s 164us/sample - loss: 1842271.1501\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11938.216 - 0s 172us/sample - loss: 10621.4233\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9098.10 - 0s 180us/sample - loss: 9748.7312\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6658.23 - 0s 188us/sample - loss: 10121.0563\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9865.38 - 0s 172us/sample - loss: 12234.6191\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10867.418 - 0s 156us/sample - loss: 10753.1074\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10305.302 - 0s 164us/sample - loss: 9804.6755\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10958.085 - 0s 172us/sample - loss: 12643188.2290\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10687.078 - 0s 156us/sample - loss: 9087.8463\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9629.25 - 0s 156us/sample - loss: 9457.0393\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9983.51 - 0s 180us/sample - loss: 10899.6221\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8105.43 - 0s 164us/sample - loss: 9158.8501\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11694.858 - 0s 164us/sample - loss: 8568132.8250\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8928.09 - 0s 172us/sample - loss: 8921.4497\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10749.226 - 0s 172us/sample - loss: 12626448.2131\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9891.35 - 0s 164us/sample - loss: 8671.7311\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 16722.425 - 0s 164us/sample - loss: 1515096.5372\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9516.52 - 0s 156us/sample - loss: 1375079.8057\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7306.61 - 0s 172us/sample - loss: 9179.7330\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8639.59 - 0s 164us/sample - loss: 9130.1899\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9711.30 - 0s 164us/sample - loss: 11653.1064\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9919.14 - 0s 172us/sample - loss: 8602608.1538\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8364.79 - 0s 156us/sample - loss: 8184.6152\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7036.53 - 0s 141us/sample - loss: 8535.9570\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9364.04 - 0s 172us/sample - loss: 11951.0406\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10363.540 - 0s 172us/sample - loss: 9200.9441\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9106.65 - 0s 179us/sample - loss: 8571889.1659\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10147.812 - 0s 164us/sample - loss: 10030.2988\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8967.02 - 0s 164us/sample - loss: 10258.5115\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6477.61 - 0s 164us/sample - loss: 10349.5338\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8303.53 - 0s 172us/sample - loss: 9106.8757\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 19835.584 - 0s 156us/sample - loss: 11374.8906\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10436.908 - 0s 172us/sample - loss: 33795.1843\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10084.543 - 0s 164us/sample - loss: 9044.5184\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6719.46 - 0s 156us/sample - loss: 8764.0452\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8562.54 - 0s 172us/sample - loss: 9501.1711\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12029.595 - 0s 172us/sample - loss: 10059.4773\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9092.99 - 0s 180us/sample - loss: 9093.6484\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10243.753 - 0s 164us/sample - loss: 33749.4954\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9233.18 - 0s 172us/sample - loss: 9273.1089\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8746.46 - 0s 148us/sample - loss: 9050.1484\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9406.99 - 0s 164us/sample - loss: 8584.2777\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12686.706 - 0s 172us/sample - loss: 9959.0828\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8303.15 - 0s 172us/sample - loss: 12645803.2925\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8315.78 - 0s 172us/sample - loss: 9781.3425\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8568.95 - 0s 164us/sample - loss: 8920.2125\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10201.295 - 0s 164us/sample - loss: 8594626.2068\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9949.54 - 0s 172us/sample - loss: 1365291.4617\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9914.16 - 0s 164us/sample - loss: 11051.1843\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8628.00 - 0s 180us/sample - loss: 1514531.8911\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9033.26 - 0s 156us/sample - loss: 9715.1567\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 104690.03 - 0s 180us/sample - loss: 34625.7363\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10650.968 - 0s 180us/sample - loss: 8570.1334\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13228.742 - 0s 180us/sample - loss: 10128.4028\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11005.364 - 0s 172us/sample - loss: 9790.8312\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6154.60 - 0s 164us/sample - loss: 8567.9277\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9996.86 - 0s 156us/sample - loss: 34455.9893\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6688.82 - 0s 172us/sample - loss: 11116.8228\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9280.74 - 0s 172us/sample - loss: 8624.3923\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9038.95 - 0s 164us/sample - loss: 9274.5459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6638.62 - 0s 164us/sample - loss: 7954.1915\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 205208.90 - 0s 156us/sample - loss: 58086.6455\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7442.46 - 0s 172us/sample - loss: 32664.0944\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10899.140 - 0s 156us/sample - loss: 9870.2947\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7613.85 - 0s 164us/sample - loss: 32922.8679\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10290.392 - 0s 203us/sample - loss: 9417.5549\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8572.51 - 0s 195us/sample - loss: 7872.2178\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9234.34 - 0s 195us/sample - loss: 1819319.7505\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10420.815 - 0s 211us/sample - loss: 10099.3921\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9541.51 - 0s 180us/sample - loss: 8724.2360\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8353.76 - 0s 203us/sample - loss: 9048.5823\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 101424.47 - 0s 180us/sample - loss: 32310.7306\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9909.72 - 0s 188us/sample - loss: 9363.0825\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11384.336 - 0s 172us/sample - loss: 9196.8198\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6988.98 - 0s 188us/sample - loss: 8770.1299\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8439.20 - 0s 187us/sample - loss: 8545.0237\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17672.421 - 0s 164us/sample - loss: 10763.9897\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 13200148.00 - 0s 180us/sample - loss: 3307029.2104\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34052372.00 - 0s 180us/sample - loss: 8518875.2422\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8793.67 - 0s 156us/sample - loss: 13168504.3970\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9456.83 - 0s 164us/sample - loss: 36693.9111\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6702.76 - 0s 172us/sample - loss: 7887.4481\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11039.060 - 0s 180us/sample - loss: 8518.1145\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 67088.664 - 0s 156us/sample - loss: 23129.0939\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8719.27 - 0s 156us/sample - loss: 10930.0159\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8286.11 - 0s 180us/sample - loss: 9403.3943\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11190.084 - 0s 172us/sample - loss: 10619.0706\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7828.30 - 0s 172us/sample - loss: 9985.2935\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7673.22 - 0s 172us/sample - loss: 10753.4536\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12183.806 - 0s 172us/sample - loss: 10591.5303\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6855.69 - 0s 156us/sample - loss: 9015.4087\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10274.450 - 0s 172us/sample - loss: 13137316.4968\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9602.13 - 0s 172us/sample - loss: 9287.7361\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8361.17 - 0s 156us/sample - loss: 9119.7207\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 103367.38 - 0s 766us/sample - loss: 32234.7887\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11129.819 - 0s 187us/sample - loss: 10523.2261\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9167.88 - 0s 172us/sample - loss: 8579807.2625\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9916.94 - 0s 156us/sample - loss: 8602.4846\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7936.72 - 0s 156us/sample - loss: 9708.9044\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9992.16 - 0s 195us/sample - loss: 9137.5298\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7509.30 - 0s 172us/sample - loss: 4391717.4902\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10221.203 - 0s 164us/sample - loss: 8914.8729\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 50360164.00 - 0s 164us/sample - loss: 12596558.3928\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9996.61 - 0s 164us/sample - loss: 55621.6423\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8238.92 - 0s 172us/sample - loss: 8582.2111\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7872.42 - 0s 156us/sample - loss: 814902.1357\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7061.16 - 0s 156us/sample - loss: 8365.7299\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10830.426 - 0s 180us/sample - loss: 8579798.4529\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8249.17 - 0s 164us/sample - loss: 9838.0178\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7439.90 - 0s 164us/sample - loss: 8505.8993\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8922.72 - 0s 172us/sample - loss: 10041.5767\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9960.90 - 0s 156us/sample - loss: 10507.1525\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11063.203 - 0s 180us/sample - loss: 9175.5968\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14042.173 - 0s 164us/sample - loss: 10303.9749\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7060.46 - 0s 172us/sample - loss: 8592.4602\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6136.14 - 0s 172us/sample - loss: 7940.9249\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9560.58 - 0s 164us/sample - loss: 8638.8337\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8759.79 - 0s 172us/sample - loss: 8407.9750\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9896.95 - 0s 172us/sample - loss: 8673.1625\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9098.87 - 0s 195us/sample - loss: 8250.1031\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8492.43 - 0s 164us/sample - loss: 13152101.9177\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7512.58 - 0s 172us/sample - loss: 13775.9106\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6808.58 - 0s 188us/sample - loss: 9441.7423\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7620.15 - 0s 180us/sample - loss: 8167.2275\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10251.236 - 0s 172us/sample - loss: 11172.0479\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 40104460.00 - 0s 164us/sample - loss: 10038269.2043\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8699.03 - 0s 156us/sample - loss: 11929.8010\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7367.33 - 0s 164us/sample - loss: 8398.9480\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9942.58 - 0s 172us/sample - loss: 7830.4205\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9727.93 - 0s 164us/sample - loss: 10694.9294\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6078.98 - 0s 164us/sample - loss: 30601.7246\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12723.220 - 0s 180us/sample - loss: 32571.0273\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9973.60 - 0s 164us/sample - loss: 9710.1953\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10347.475 - 0s 164us/sample - loss: 56015.7439\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7965.72 - 0s 172us/sample - loss: 7771.1691\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34091556.00 - 0s 164us/sample - loss: 8552867.1907\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8897.68 - 0s 164us/sample - loss: 33073.5442\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 197458.21 - 0s 164us/sample - loss: 54933.3795\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9743.54 - 0s 156us/sample - loss: 12466546.0466\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9019.35 - 0s 164us/sample - loss: 9662.6484\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9830.62 - 0s 172us/sample - loss: 30659.7491\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8630.63 - 0s 188us/sample - loss: 8757.2881\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10765.959 - 0s 148us/sample - loss: 9778.7621\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7744.49 - 0s 180us/sample - loss: 8559097.6603\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 52524196.00 - 0s 180us/sample - loss: 13136688.9620\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10236.085 - 0s 172us/sample - loss: 32503.7461\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11076.023 - 0s 172us/sample - loss: 10575.1252\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9168.40 - 0s 156us/sample - loss: 11605.8872\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9395.97 - 0s 164us/sample - loss: 8554453.5684\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6010560.000 - 0s 164us/sample - loss: 14116055.2417\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15819.734 - 0s 172us/sample - loss: 10569.1108\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8178.19 - 0s 172us/sample - loss: 32713.3328\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8480.25 - 0s 188us/sample - loss: 9102.7510\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6758.77 - 0s 164us/sample - loss: 8153.3280\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10010.083 - 0s 164us/sample - loss: 9878.6208\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8812.07 - 0s 164us/sample - loss: 21756.2551\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8749.19 - 0s 156us/sample - loss: 9824.9476\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11058.712 - 0s 164us/sample - loss: 8932.0867\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9482.75 - 0s 164us/sample - loss: 8529.5852\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8713.44 - 0s 180us/sample - loss: 31199.2012\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8325.25 - 0s 164us/sample - loss: 9066.6360\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7791.78 - 0s 172us/sample - loss: 32752.6638\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8671.20 - 0s 164us/sample - loss: 9883.5791\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 97177.101 - 0s 164us/sample - loss: 31749.9480\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7879.58 - 0s 156us/sample - loss: 30679.2510\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8604.27 - 0s 164us/sample - loss: 8536178.0842\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7094.84 - 0s 172us/sample - loss: 8204.8599\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7031.54 - 0s 164us/sample - loss: 8513.3918\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 12287.056 - 0s 172us/sample - loss: 11144.6248\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9414.84 - 0s 164us/sample - loss: 10119.1755\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6191.38 - 0s 164us/sample - loss: 6849.3616\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 96760.234 - 0s 164us/sample - loss: 31176.9893\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8083.41 - 0s 172us/sample - loss: 3181876.7112\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8034.59 - 0s 156us/sample - loss: 11001.9010\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8127.89 - 0s 180us/sample - loss: 31488.2294\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17605.533 - 0s 172us/sample - loss: 9626.0271\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10839.523 - 0s 164us/sample - loss: 8428.3977\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7906.18 - 0s 180us/sample - loss: 8323.7543\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34039308.00 - 0s 164us/sample - loss: 8516591.5309\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10004.617 - 0s 164us/sample - loss: 10637.9575\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10826.678 - 0s 172us/sample - loss: 8514438.3887\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7183.74 - 0s 164us/sample - loss: 10926.5649\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9681.39 - 0s 164us/sample - loss: 16052.9961\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 15166.675 - 0s 172us/sample - loss: 32804.2766\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 17520.121 - 0s 180us/sample - loss: 11048.5559\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10964.835 - 0s 188us/sample - loss: 10012.6772\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10043.824 - 0s 172us/sample - loss: 10771.9460\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9086.48 - 0s 195us/sample - loss: 10744.8757\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 14320.228 - 0s 172us/sample - loss: 10655.1080\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6825.57 - 0s 180us/sample - loss: 7699.5691\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 8639.51 - 0s 148us/sample - loss: 8932.8665\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10305.251 - 0s 172us/sample - loss: 8082.6611\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7898.20 - 0s 180us/sample - loss: 8310.8606\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10141.797 - 0s 172us/sample - loss: 9117.3922\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9710.93 - 0s 180us/sample - loss: 9004.9547\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9913.28 - 0s 164us/sample - loss: 10551.2100\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 5689.10 - 0s 180us/sample - loss: 7334.1465\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 101205.78 - 0s 172us/sample - loss: 32100.2427\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 5817.79 - 0s 180us/sample - loss: 8256.0140\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 4981.18 - 0s 172us/sample - loss: 30625.5897\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10976.627 - 0s 164us/sample - loss: 8948.3560\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9298.97 - 0s 188us/sample - loss: 9300.4233\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9672.21 - 0s 1ms/sample - loss: 8810.5806\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7589.95 - 0s 227us/sample - loss: 8673.1702\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6704.95 - 0s 297us/sample - loss: 8563109.5154\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 11411.170 - 0s 211us/sample - loss: 9058.0757\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8534.27 - 0s 219us/sample - loss: 9509.6188\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9457.28 - 0s 211us/sample - loss: 8822.4874\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9744.00 - 0s 195us/sample - loss: 7927.3434\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7632.31 - 0s 180us/sample - loss: 8333.2889\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6789.42 - 0s 180us/sample - loss: 10197.4946\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8918.36 - 0s 195us/sample - loss: 7926.1942\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7423.16 - 0s 188us/sample - loss: 32770.3083\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 10933.073 - 0s 195us/sample - loss: 9704.2582\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6733.21 - 0s 172us/sample - loss: 7796.7328\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8430.76 - 0s 211us/sample - loss: 9936.2529\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 6258.63 - 0s 188us/sample - loss: 7637.3066\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9475.86 - 0s 195us/sample - loss: 8654.6997\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8109.99 - 0s 312us/sample - loss: 9102.5249\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 9760.35 - 0s 188us/sample - loss: 1509178.4836\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 8781.77 - 0s 164us/sample - loss: 9009.4771\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 5588.88 - 0s 180us/sample - loss: 8826.1996\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7633.53 - 0s 187us/sample - loss: 7866.2971\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 7870.16 - 0s 180us/sample - loss: 8517.8929\n",
      "Train on 128 samples\n",
      "128/128 [==============================] - ETA: 0s - loss: 34142664.00 - 0s 203us/sample - loss: 8544256.2959\n",
      "Train on 128 samples\n",
      " 32/128 [======>.......................] - ETA: 0s - loss: 7227.7959"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-589-45afd9360e5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mfake_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfake_batch_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_flights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(1000):\n",
    "        r_f = np.random.choice(true_data.shape[0], \n",
    "                                            batch_size_true, \n",
    "                                            replace=False)\n",
    "        true_flights = true_data.loc[r_f].values\n",
    "        us_id = true_data.loc[r_f]['User_ID'].values\n",
    "        \n",
    "        fake_batch_df = pd.DataFrame(us_id, columns=['User_ID']).join(user_data, on = 'User_ID')\n",
    "        fake_batch = fake_batch_df.values\n",
    "        \n",
    "        G.fit(fake_batch.astype('float32'), true_flights.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0    213\n",
       "1      1\n",
       "2     96\n",
       "3    213\n",
       "4    565\n",
       "..   ...\n",
       "123  201\n",
       "124    1\n",
       "125  210\n",
       "126  242\n",
       "127   89\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(true_flights[:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>109.358582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>103.775558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>110.010330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111.509903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>94.827812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>92.913284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>145.587753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>126.711304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>150.940582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>149.817612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0    109.358582\n",
       "1    103.775558\n",
       "2    110.010330\n",
       "3    111.509903\n",
       "4     94.827812\n",
       "..          ...\n",
       "123   92.913284\n",
       "124  145.587753\n",
       "125  126.711304\n",
       "126  150.940582\n",
       "127  149.817612\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(G.predict(fake_batch.astype('float32'))[:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192 samples\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025B05F9DA68> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025B05F9DA68> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "192/192 [==============================] - ETA: 4s - loss: 6639.5444 - accuracy: 0.437 - 1s 5ms/sample - loss: 6099.4882 - accuracy: 0.3750\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 5700.5850 - accuracy: 0.375 - 0s 188us/sample - loss: 4229.2741 - accuracy: 0.4219\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 3358.9302 - accuracy: 0.531 - 0s 161us/sample - loss: 2940.3015 - accuracy: 0.5156\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 4142.2441 - accuracy: 0.500 - 0s 172us/sample - loss: 2611.3655 - accuracy: 0.5521\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 3447.3462 - accuracy: 0.375 - 0s 188us/sample - loss: 2562.3595 - accuracy: 0.4948\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1350.0874 - accuracy: 0.718 - 0s 161us/sample - loss: 2336.5942 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 2631.6333 - accuracy: 0.500 - 0s 172us/sample - loss: 1876.8910 - accuracy: 0.5469\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 2403.6101 - accuracy: 0.406 - 0s 161us/sample - loss: 2320.9228 - accuracy: 0.5000\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 979.0688 - accuracy: 0.71 - 0s 167us/sample - loss: 2261.0309 - accuracy: 0.5208\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 2061.1921 - accuracy: 0.468 - 0s 156us/sample - loss: 1758.2990 - accuracy: 0.5104\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1779.4911 - accuracy: 0.687 - 0s 146us/sample - loss: 1414.5767 - accuracy: 0.5729\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 2290.7810 - accuracy: 0.500 - 0s 177us/sample - loss: 1792.9266 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1587.5739 - accuracy: 0.531 - 0s 182us/sample - loss: 1347.6937 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1513.9095 - accuracy: 0.406 - 0s 177us/sample - loss: 1388.2616 - accuracy: 0.5104\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1645.9771 - accuracy: 0.625 - 0s 198us/sample - loss: 1487.3659 - accuracy: 0.5833\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1516.4348 - accuracy: 0.625 - 0s 177us/sample - loss: 1244.5658 - accuracy: 0.5677\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1103.5999 - accuracy: 0.562 - 0s 161us/sample - loss: 957.3695 - accuracy: 0.6250\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1367.7229 - accuracy: 0.562 - 0s 156us/sample - loss: 1224.8382 - accuracy: 0.5677\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1413.7654 - accuracy: 0.687 - 0s 151us/sample - loss: 1204.4601 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 476.5488 - accuracy: 0.62 - 0s 172us/sample - loss: 1054.0202 - accuracy: 0.5312\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 761.5165 - accuracy: 0.46 - 0s 198us/sample - loss: 834.2332 - accuracy: 0.5156\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 804.3516 - accuracy: 0.56 - 0s 182us/sample - loss: 749.0717 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1097.5598 - accuracy: 0.500 - 0s 161us/sample - loss: 837.5114 - accuracy: 0.5365\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 971.9327 - accuracy: 0.59 - 0s 177us/sample - loss: 946.9627 - accuracy: 0.5312\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1138.3579 - accuracy: 0.656 - 0s 156us/sample - loss: 1064.8707 - accuracy: 0.5469\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1366.5334 - accuracy: 0.468 - 0s 177us/sample - loss: 753.5177 - accuracy: 0.5833\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 771.3406 - accuracy: 0.62 - 0s 146us/sample - loss: 948.8004 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 285.2201 - accuracy: 0.71 - 0s 177us/sample - loss: 812.2861 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 887.5770 - accuracy: 0.59 - 0s 255us/sample - loss: 916.0637 - accuracy: 0.4896\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 831.5571 - accuracy: 0.59 - 0s 182us/sample - loss: 715.8126 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 980.7156 - accuracy: 0.40 - 0s 167us/sample - loss: 811.7136 - accuracy: 0.5469\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 860.5466 - accuracy: 0.53 - 0s 188us/sample - loss: 777.8035 - accuracy: 0.4948\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 446.8167 - accuracy: 0.62 - 0s 172us/sample - loss: 687.7513 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 822.1925 - accuracy: 0.59 - 0s 167us/sample - loss: 726.6187 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 1039.7515 - accuracy: 0.531 - 0s 156us/sample - loss: 732.0163 - accuracy: 0.5365\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 545.6782 - accuracy: 0.50 - 0s 146us/sample - loss: 693.8087 - accuracy: 0.5052\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 464.8410 - accuracy: 0.62 - 0s 151us/sample - loss: 471.0298 - accuracy: 0.6354\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 776.8110 - accuracy: 0.53 - 0s 141us/sample - loss: 595.0159 - accuracy: 0.5938\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 777.6507 - accuracy: 0.43 - 0s 146us/sample - loss: 785.5907 - accuracy: 0.4844\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 506.6700 - accuracy: 0.56 - 0s 177us/sample - loss: 516.9904 - accuracy: 0.5104\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 774.7305 - accuracy: 0.50 - 0s 182us/sample - loss: 618.3350 - accuracy: 0.5000\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 933.0852 - accuracy: 0.50 - 0s 167us/sample - loss: 654.6394 - accuracy: 0.5208\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 369.4621 - accuracy: 0.53 - 0s 182us/sample - loss: 589.6589 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 696.5739 - accuracy: 0.53 - 0s 193us/sample - loss: 628.4017 - accuracy: 0.5938\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 816.1692 - accuracy: 0.46 - 0s 187us/sample - loss: 573.4752 - accuracy: 0.5521\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 750.1177 - accuracy: 0.50 - 0s 156us/sample - loss: 574.6596 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 423.5343 - accuracy: 0.50 - 0s 208us/sample - loss: 511.9026 - accuracy: 0.5104\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 674.9973 - accuracy: 0.53 - 0s 177us/sample - loss: 575.2365 - accuracy: 0.5312\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 609.3331 - accuracy: 0.56 - 0s 167us/sample - loss: 445.3925 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 251.3181 - accuracy: 0.78 - 0s 208us/sample - loss: 469.3693 - accuracy: 0.5833\n"
     ]
    }
   ],
   "source": [
    "for j in range(50):\n",
    "    random_ids = np.random.choice(ids_list, batch_size_noise, replace=False)\n",
    "    fake_batch_df = pd.DataFrame(random_ids, columns=['User_ID']).join(user_data, on = 'User_ID')\n",
    "    fake_batch = fake_batch_df.values\n",
    "\n",
    "    fake_flights = G.predict(fake_batch.astype('float32'))\n",
    "\n",
    "    x = np.concatenate((true_flights, fake_flights))\n",
    "    y = np.ones([batch_size_true + batch_size_noise, 1])\n",
    "    y[batch_size_true:, :] = 0\n",
    "    d_loss = D.fit(x.astype('float32'), y.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37724a7011e84f77929f3a62e178cd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 486.4697 - accuracy: 0.59 - 0s 167us/sample - loss: 476.3072 - accuracy: 0.5365\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 387.4220 - accuracy: 0.56 - 0s 151us/sample - loss: 335.6163 - accuracy: 0.5781\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 593.2582 - accuracy: 0.43 - 0s 161us/sample - loss: 463.4562 - accuracy: 0.5208\n",
      "Train on 64 samples\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025AF62C3EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025AF62C3EE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "64/64 [==============================] - ETA: 1s - loss: 372.3060 - accuracy: 0.62 - 1s 20ms/sample - loss: 637.7457 - accuracy: 0.5938\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 272.8517 - accuracy: 0.68 - 0s 406us/sample - loss: 308.5241 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 188.0918 - accuracy: 0.78 - 0s 391us/sample - loss: 254.0525 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 158.6742 - accuracy: 0.78 - 0s 391us/sample - loss: 177.1413 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 218.4684 - accuracy: 0.62 - 0s 344us/sample - loss: 186.3741 - accuracy: 0.6875\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 355.0400 - accuracy: 0.71 - 0s 266us/sample - loss: 463.9511 - accuracy: 0.5729\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 420.6389 - accuracy: 0.59 - 0s 188us/sample - loss: 412.0653 - accuracy: 0.5677\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 365.0443 - accuracy: 0.53 - 0s 177us/sample - loss: 446.6403 - accuracy: 0.5052\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 137.7495 - accuracy: 0.78 - 0s 375us/sample - loss: 201.5640 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 74.2924 - accuracy: 0.843 - 0s 343us/sample - loss: 126.0498 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 307.5672 - accuracy: 0.62 - 0s 344us/sample - loss: 262.0424 - accuracy: 0.6094\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 134.0202 - accuracy: 0.78 - 0s 360us/sample - loss: 160.5271 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 290.4702 - accuracy: 0.78 - 0s 359us/sample - loss: 185.1084 - accuracy: 0.7969\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 383.9619 - accuracy: 0.62 - 0s 167us/sample - loss: 337.5195 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 304.2361 - accuracy: 0.50 - 0s 229us/sample - loss: 393.4240 - accuracy: 0.5208\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 289.2624 - accuracy: 0.56 - 0s 182us/sample - loss: 318.7880 - accuracy: 0.5677\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 177.7387 - accuracy: 0.68 - 0s 297us/sample - loss: 134.2103 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 211.1390 - accuracy: 0.56 - 0s 359us/sample - loss: 157.8369 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 73.5086 - accuracy: 0.781 - 0s 359us/sample - loss: 90.7357 - accuracy: 0.7812\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 168.5349 - accuracy: 0.75 - 0s 328us/sample - loss: 150.6250 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 191.3220 - accuracy: 0.75 - 0s 359us/sample - loss: 127.1883 - accuracy: 0.8281\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 566.8613 - accuracy: 0.50 - 0s 156us/sample - loss: 360.2658 - accuracy: 0.6146\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 356.9377 - accuracy: 0.65 - 0s 146us/sample - loss: 360.0548 - accuracy: 0.5469\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 451.6942 - accuracy: 0.46 - 0s 161us/sample - loss: 327.5209 - accuracy: 0.5781\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 207.7668 - accuracy: 0.59 - 0s 313us/sample - loss: 171.4208 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 154.2638 - accuracy: 0.87 - 0s 375us/sample - loss: 136.7230 - accuracy: 0.8594\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 254.4833 - accuracy: 0.59 - 0s 359us/sample - loss: 204.0472 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 150.3401 - accuracy: 0.78 - 0s 312us/sample - loss: 138.2948 - accuracy: 0.7969\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 154.9951 - accuracy: 0.62 - 0s 328us/sample - loss: 132.8214 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 487.5936 - accuracy: 0.50 - 0s 161us/sample - loss: 406.9146 - accuracy: 0.5312\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 283.1544 - accuracy: 0.56 - 0s 182us/sample - loss: 283.4331 - accuracy: 0.5990\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 326.3274 - accuracy: 0.59 - 0s 182us/sample - loss: 380.6610 - accuracy: 0.5521\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 40.7725 - accuracy: 0.843 - 0s 344us/sample - loss: 83.0035 - accuracy: 0.7812\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 110.9115 - accuracy: 0.84 - 0s 375us/sample - loss: 160.6841 - accuracy: 0.7969\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 205.2516 - accuracy: 0.65 - 0s 359us/sample - loss: 162.3779 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 246.1038 - accuracy: 0.71 - 0s 328us/sample - loss: 158.9237 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 167.8978 - accuracy: 0.68 - 0s 328us/sample - loss: 175.5706 - accuracy: 0.6875\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 312.4765 - accuracy: 0.59 - 0s 167us/sample - loss: 300.5453 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 620.9792 - accuracy: 0.56 - 0s 167us/sample - loss: 408.6548 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 273.8846 - accuracy: 0.68 - 0s 167us/sample - loss: 248.6786 - accuracy: 0.6615\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 152.7752 - accuracy: 0.62 - 0s 328us/sample - loss: 102.7892 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 260.0483 - accuracy: 0.81 - 0s 328us/sample - loss: 190.9701 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 256.4082 - accuracy: 0.62 - 0s 406us/sample - loss: 203.9349 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 155.1753 - accuracy: 0.68 - 0s 344us/sample - loss: 136.3926 - accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 177.9473 - accuracy: 0.59 - 0s 359us/sample - loss: 139.9692 - accuracy: 0.6562\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 338.9975 - accuracy: 0.53 - 0s 193us/sample - loss: 291.9880 - accuracy: 0.6042\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 429.5461 - accuracy: 0.40 - 0s 187us/sample - loss: 323.5788 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 351.9296 - accuracy: 0.65 - 0s 187us/sample - loss: 257.2278 - accuracy: 0.6042\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 209.9065 - accuracy: 0.62 - 0s 375us/sample - loss: 174.8917 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 226.8800 - accuracy: 0.62 - 0s 328us/sample - loss: 225.9062 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 278.8798 - accuracy: 0.59 - 0s 313us/sample - loss: 217.5009 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 107.0898 - accuracy: 0.75 - 0s 344us/sample - loss: 115.1564 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 65.1774 - accuracy: 0.781 - 0s 359us/sample - loss: 101.1901 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 188.9896 - accuracy: 0.65 - 0s 177us/sample - loss: 300.9547 - accuracy: 0.5781\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 305.6098 - accuracy: 0.53 - 0s 156us/sample - loss: 334.1771 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 233.1024 - accuracy: 0.62 - 0s 156us/sample - loss: 263.2110 - accuracy: 0.5833\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 28.6741 - accuracy: 0.875 - 0s 390us/sample - loss: 61.6455 - accuracy: 0.7812\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 197.6594 - accuracy: 0.65 - 0s 391us/sample - loss: 157.6300 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 117.3193 - accuracy: 0.62 - 0s 360us/sample - loss: 149.7523 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 189.6988 - accuracy: 0.65 - 0s 344us/sample - loss: 138.5521 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 42.9511 - accuracy: 0.843 - 0s 375us/sample - loss: 78.2004 - accuracy: 0.7969\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 312.3106 - accuracy: 0.53 - 0s 162us/sample - loss: 214.0874 - accuracy: 0.6146\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 208.1855 - accuracy: 0.59 - 0s 161us/sample - loss: 260.1394 - accuracy: 0.5729\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 233.0334 - accuracy: 0.59 - 0s 182us/sample - loss: 292.1521 - accuracy: 0.5677\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 101.8303 - accuracy: 0.71 - 0s 359us/sample - loss: 164.0924 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 181.1472 - accuracy: 0.59 - 0s 422us/sample - loss: 155.6903 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 141.1690 - accuracy: 0.65 - 0s 1ms/sample - loss: 101.8938 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 97.9300 - accuracy: 0.718 - 0s 359us/sample - loss: 93.5210 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 149.3782 - accuracy: 0.65 - 0s 329us/sample - loss: 149.3153 - accuracy: 0.7031\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 258.3951 - accuracy: 0.65 - 0s 177us/sample - loss: 264.9461 - accuracy: 0.6198\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 355.4834 - accuracy: 0.50 - 0s 177us/sample - loss: 258.9278 - accuracy: 0.5781\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 234.9041 - accuracy: 0.53 - 0s 172us/sample - loss: 266.2452 - accuracy: 0.5521\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 127.5414 - accuracy: 0.62 - 0s 359us/sample - loss: 131.5322 - accuracy: 0.5938\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 100.1853 - accuracy: 0.65 - 0s 344us/sample - loss: 100.4117 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 65.7398 - accuracy: 0.625 - 0s 359us/sample - loss: 102.7436 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 56.2642 - accuracy: 0.812 - 0s 390us/sample - loss: 103.9856 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 46.6032 - accuracy: 0.812 - 0s 375us/sample - loss: 66.3100 - accuracy: 0.8125\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 194.2623 - accuracy: 0.65 - 0s 161us/sample - loss: 238.9136 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 435.6798 - accuracy: 0.46 - 0s 167us/sample - loss: 254.2075 - accuracy: 0.5365\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 156.1909 - accuracy: 0.56 - 0s 161us/sample - loss: 217.6139 - accuracy: 0.5833\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 251.7979 - accuracy: 0.62 - 0s 391us/sample - loss: 193.2765 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 148.6765 - accuracy: 0.56 - 0s 375us/sample - loss: 126.4691 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 133.7336 - accuracy: 0.68 - 0s 359us/sample - loss: 127.0730 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 105.7956 - accuracy: 0.78 - 0s 359us/sample - loss: 153.5194 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 143.7174 - accuracy: 0.68 - 0s 344us/sample - loss: 161.6535 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 275.2379 - accuracy: 0.59 - 0s 162us/sample - loss: 255.8319 - accuracy: 0.5677\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 207.6923 - accuracy: 0.46 - 0s 161us/sample - loss: 251.0868 - accuracy: 0.5156\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 247.1573 - accuracy: 0.62 - 0s 167us/sample - loss: 216.6613 - accuracy: 0.5885\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 142.3665 - accuracy: 0.59 - 0s 328us/sample - loss: 117.9042 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 175.7313 - accuracy: 0.62 - 0s 313us/sample - loss: 130.9811 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 175.8874 - accuracy: 0.75 - 0s 359us/sample - loss: 118.8022 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 51.2911 - accuracy: 0.812 - 0s 344us/sample - loss: 129.4423 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 124.1185 - accuracy: 0.62 - 0s 375us/sample - loss: 137.6926 - accuracy: 0.6562\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 274.1153 - accuracy: 0.62 - 0s 161us/sample - loss: 229.6765 - accuracy: 0.5781\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 145.1190 - accuracy: 0.56 - 0s 151us/sample - loss: 189.3988 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 321.7069 - accuracy: 0.53 - 0s 146us/sample - loss: 196.6638 - accuracy: 0.5729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 165.8200 - accuracy: 0.65 - 0s 375us/sample - loss: 139.5580 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 80.4523 - accuracy: 0.750 - 0s 328us/sample - loss: 111.6322 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 88.1946 - accuracy: 0.656 - 0s 328us/sample - loss: 118.8984 - accuracy: 0.5938\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 105.8588 - accuracy: 0.78 - 0s 359us/sample - loss: 113.2695 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 92.5976 - accuracy: 0.687 - 0s 359us/sample - loss: 62.8890 - accuracy: 0.7656\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 280.6440 - accuracy: 0.46 - 0s 167us/sample - loss: 221.9014 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 248.2308 - accuracy: 0.53 - 0s 156us/sample - loss: 169.4531 - accuracy: 0.6510\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 136.0273 - accuracy: 0.68 - 0s 167us/sample - loss: 164.0359 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 67.7784 - accuracy: 0.781 - 0s 312us/sample - loss: 97.2448 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 86.8945 - accuracy: 0.750 - 0s 328us/sample - loss: 109.6211 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 155.8465 - accuracy: 0.59 - 0s 344us/sample - loss: 117.0511 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 70.6476 - accuracy: 0.812 - 0s 344us/sample - loss: 101.9401 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 86.2678 - accuracy: 0.781 - 0s 375us/sample - loss: 105.2704 - accuracy: 0.6719\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 224.1588 - accuracy: 0.56 - 0s 161us/sample - loss: 172.3484 - accuracy: 0.5677\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 262.4064 - accuracy: 0.53 - 0s 167us/sample - loss: 214.7616 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 374.6305 - accuracy: 0.50 - 0s 187us/sample - loss: 199.9058 - accuracy: 0.5573\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 90.2076 - accuracy: 0.625 - 0s 344us/sample - loss: 65.4540 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 162.3524 - accuracy: 0.59 - 0s 375us/sample - loss: 120.5567 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 97.7267 - accuracy: 0.656 - 0s 360us/sample - loss: 60.6529 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 126.9681 - accuracy: 0.75 - 0s 359us/sample - loss: 122.5238 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 64.3757 - accuracy: 0.750 - 0s 344us/sample - loss: 50.9126 - accuracy: 0.7656\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 159.6337 - accuracy: 0.59 - 0s 161us/sample - loss: 195.9260 - accuracy: 0.5729\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 183.3144 - accuracy: 0.56 - 0s 161us/sample - loss: 184.2254 - accuracy: 0.5990\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 112.2795 - accuracy: 0.59 - 0s 172us/sample - loss: 153.7151 - accuracy: 0.5625\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 88.9106 - accuracy: 0.687 - 0s 313us/sample - loss: 109.6894 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 161.0526 - accuracy: 0.56 - 0s 344us/sample - loss: 161.3594 - accuracy: 0.6094\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 66.1142 - accuracy: 0.750 - 0s 328us/sample - loss: 59.2932 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 50.5012 - accuracy: 0.718 - 0s 344us/sample - loss: 62.3895 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 63.2972 - accuracy: 0.812 - 0s 360us/sample - loss: 68.4237 - accuracy: 0.7812\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 93.8510 - accuracy: 0.687 - 0s 161us/sample - loss: 140.8076 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 389.2032 - accuracy: 0.34 - 0s 172us/sample - loss: 203.5335 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 203.8337 - accuracy: 0.56 - 0s 182us/sample - loss: 162.3343 - accuracy: 0.5625\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 83.0743 - accuracy: 0.656 - 0s 359us/sample - loss: 74.6778 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 121.6719 - accuracy: 0.59 - 0s 344us/sample - loss: 130.1251 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 50.5537 - accuracy: 0.781 - 0s 359us/sample - loss: 63.3214 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 90.6240 - accuracy: 0.718 - 0s 312us/sample - loss: 115.3026 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 79.0066 - accuracy: 0.812 - 0s 328us/sample - loss: 74.4962 - accuracy: 0.7656\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 146.8187 - accuracy: 0.59 - 0s 156us/sample - loss: 151.9392 - accuracy: 0.5938\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 97.3439 - accuracy: 0.656 - 0s 146us/sample - loss: 132.7147 - accuracy: 0.6458\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 172.8356 - accuracy: 0.59 - 0s 167us/sample - loss: 157.7390 - accuracy: 0.5469\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 128.2203 - accuracy: 0.68 - 0s 328us/sample - loss: 89.8995 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 74.9767 - accuracy: 0.687 - 0s 359us/sample - loss: 65.0932 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 101.9454 - accuracy: 0.62 - 0s 328us/sample - loss: 80.5134 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 74.8573 - accuracy: 0.781 - 0s 344us/sample - loss: 81.8561 - accuracy: 0.7812\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 114.0638 - accuracy: 0.62 - 0s 359us/sample - loss: 79.4509 - accuracy: 0.7344\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 99.9801 - accuracy: 0.718 - 0s 182us/sample - loss: 158.7985 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 182.6459 - accuracy: 0.56 - 0s 172us/sample - loss: 169.5237 - accuracy: 0.5781\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 127.8950 - accuracy: 0.62 - 0s 151us/sample - loss: 144.8681 - accuracy: 0.5833\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 45.4922 - accuracy: 0.656 - 0s 344us/sample - loss: 86.9799 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 74.3552 - accuracy: 0.750 - 0s 298us/sample - loss: 68.6303 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 45.1712 - accuracy: 0.750 - 0s 297us/sample - loss: 72.8889 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 64.9529 - accuracy: 0.656 - 0s 328us/sample - loss: 82.2605 - accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 43.9228 - accuracy: 0.875 - 0s 359us/sample - loss: 50.7280 - accuracy: 0.8438\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 143.7411 - accuracy: 0.53 - 0s 151us/sample - loss: 158.2512 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 105.8515 - accuracy: 0.71 - 0s 182us/sample - loss: 152.7791 - accuracy: 0.6094\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 122.7077 - accuracy: 0.59 - 0s 177us/sample - loss: 135.3153 - accuracy: 0.5885\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 60.5975 - accuracy: 0.812 - 0s 328us/sample - loss: 79.6441 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 61.7338 - accuracy: 0.750 - 0s 344us/sample - loss: 69.1635 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 92.8348 - accuracy: 0.750 - 0s 296us/sample - loss: 71.9434 - accuracy: 0.8125\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 56.8487 - accuracy: 0.593 - 0s 312us/sample - loss: 53.0008 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 112.6328 - accuracy: 0.71 - 0s 313us/sample - loss: 72.6230 - accuracy: 0.7344\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 228.5685 - accuracy: 0.56 - 0s 156us/sample - loss: 140.6056 - accuracy: 0.5521\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 146.9279 - accuracy: 0.53 - 0s 177us/sample - loss: 130.0716 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 102.2427 - accuracy: 0.56 - 0s 156us/sample - loss: 123.3087 - accuracy: 0.5990\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 77.9953 - accuracy: 0.718 - 0s 375us/sample - loss: 97.9949 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 45.1916 - accuracy: 0.843 - 0s 359us/sample - loss: 65.3362 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 75.9084 - accuracy: 0.687 - 0s 344us/sample - loss: 72.4673 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 18.8848 - accuracy: 0.875 - 0s 359us/sample - loss: 24.1845 - accuracy: 0.8594\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 115.8910 - accuracy: 0.65 - 0s 359us/sample - loss: 99.6045 - accuracy: 0.6406\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 145.8716 - accuracy: 0.53 - 0s 156us/sample - loss: 135.9562 - accuracy: 0.5052\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 107.0555 - accuracy: 0.68 - 0s 167us/sample - loss: 172.5776 - accuracy: 0.5521\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 184.0255 - accuracy: 0.56 - 0s 182us/sample - loss: 136.0189 - accuracy: 0.5885\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 52.6040 - accuracy: 0.656 - 0s 359us/sample - loss: 70.9339 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 78.5967 - accuracy: 0.593 - 0s 406us/sample - loss: 68.0760 - accuracy: 0.6094\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 70.8074 - accuracy: 0.750 - 0s 359us/sample - loss: 57.7807 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 102.9984 - accuracy: 0.59 - 0s 407us/sample - loss: 80.4235 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 56.2845 - accuracy: 0.750 - 0s 312us/sample - loss: 68.8597 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 185.2973 - accuracy: 0.59 - 0s 156us/sample - loss: 118.2992 - accuracy: 0.6302\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 72.8844 - accuracy: 0.781 - 0s 162us/sample - loss: 114.7218 - accuracy: 0.6094\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 136.8469 - accuracy: 0.56 - 0s 161us/sample - loss: 126.9248 - accuracy: 0.5781\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 60.2637 - accuracy: 0.718 - 0s 297us/sample - loss: 58.0479 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 131.5339 - accuracy: 0.59 - 0s 297us/sample - loss: 118.1782 - accuracy: 0.5781\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 53.0469 - accuracy: 0.718 - 0s 328us/sample - loss: 61.8413 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 102.6300 - accuracy: 0.71 - 0s 313us/sample - loss: 93.1034 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 30.2345 - accuracy: 0.843 - 0s 328us/sample - loss: 35.4683 - accuracy: 0.8125\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 144.0508 - accuracy: 0.40 - 0s 156us/sample - loss: 120.0761 - accuracy: 0.5833\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 183.5087 - accuracy: 0.40 - 0s 167us/sample - loss: 129.4815 - accuracy: 0.5885\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 117.3945 - accuracy: 0.50 - 0s 167us/sample - loss: 107.4214 - accuracy: 0.5521\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 73.6018 - accuracy: 0.625 - 0s 313us/sample - loss: 100.1897 - accuracy: 0.5781\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 80.5796 - accuracy: 0.656 - 0s 375us/sample - loss: 62.3868 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 131.1923 - accuracy: 0.59 - 0s 359us/sample - loss: 85.5961 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 77.0475 - accuracy: 0.781 - 0s 313us/sample - loss: 63.9757 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 30.2248 - accuracy: 0.812 - 0s 328us/sample - loss: 45.8159 - accuracy: 0.7969\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 184.3030 - accuracy: 0.56 - 0s 141us/sample - loss: 117.8175 - accuracy: 0.6042\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 93.6540 - accuracy: 0.593 - 0s 167us/sample - loss: 114.6304 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 133.9254 - accuracy: 0.59 - 0s 172us/sample - loss: 107.6158 - accuracy: 0.6094\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 66.8171 - accuracy: 0.718 - 0s 375us/sample - loss: 79.7746 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 94.8382 - accuracy: 0.656 - 0s 328us/sample - loss: 97.1319 - accuracy: 0.6250\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 77.7176 - accuracy: 0.656 - 0s 344us/sample - loss: 72.1807 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 29.9716 - accuracy: 0.781 - 0s 375us/sample - loss: 62.1798 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 47.5967 - accuracy: 0.718 - 0s 328us/sample - loss: 50.3404 - accuracy: 0.6719\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 82.5787 - accuracy: 0.468 - 0s 156us/sample - loss: 116.1908 - accuracy: 0.5833\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 127.4837 - accuracy: 0.50 - 0s 161us/sample - loss: 131.9479 - accuracy: 0.4948\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 106.7270 - accuracy: 0.56 - 0s 151us/sample - loss: 126.1078 - accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 32.3078 - accuracy: 0.656 - 0s 359us/sample - loss: 36.9041 - accuracy: 0.7031\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 42.5149 - accuracy: 0.812 - 0s 312us/sample - loss: 38.0486 - accuracy: 0.8281\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 61.3719 - accuracy: 0.687 - 0s 312us/sample - loss: 38.2752 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 57.1555 - accuracy: 0.562 - 0s 313us/sample - loss: 57.6171 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 85.6864 - accuracy: 0.625 - 0s 328us/sample - loss: 56.4733 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 134.7971 - accuracy: 0.34 - 0s 156us/sample - loss: 103.0565 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 107.8311 - accuracy: 0.65 - 0s 167us/sample - loss: 123.7589 - accuracy: 0.5833\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 84.6398 - accuracy: 0.531 - 0s 156us/sample - loss: 113.3354 - accuracy: 0.5365\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 49.1109 - accuracy: 0.656 - 0s 312us/sample - loss: 70.6038 - accuracy: 0.6250\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 64.3915 - accuracy: 0.625 - 0s 313us/sample - loss: 57.4070 - accuracy: 0.6250\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 34.6578 - accuracy: 0.812 - 0s 297us/sample - loss: 63.9239 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 47.1774 - accuracy: 0.656 - 0s 328us/sample - loss: 46.6251 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 76.4876 - accuracy: 0.718 - 0s 328us/sample - loss: 56.8252 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 121.1323 - accuracy: 0.40 - 0s 177us/sample - loss: 85.5593 - accuracy: 0.6094\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 78.2737 - accuracy: 0.625 - 0s 161us/sample - loss: 121.3853 - accuracy: 0.5260\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 105.5894 - accuracy: 0.53 - 0s 167us/sample - loss: 100.3891 - accuracy: 0.5312\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 7.2079 - accuracy: 0.84 - 0s 313us/sample - loss: 38.9698 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 17.9957 - accuracy: 0.875 - 0s 328us/sample - loss: 44.0334 - accuracy: 0.8125\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 15.1460 - accuracy: 0.875 - 0s 359us/sample - loss: 18.4561 - accuracy: 0.8594\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 23.0531 - accuracy: 0.718 - 0s 359us/sample - loss: 38.8454 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 36.1350 - accuracy: 0.750 - 0s 359us/sample - loss: 47.5421 - accuracy: 0.6719\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 56.8591 - accuracy: 0.687 - 0s 172us/sample - loss: 93.2256 - accuracy: 0.5990\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 128.3445 - accuracy: 0.59 - 0s 172us/sample - loss: 87.3338 - accuracy: 0.6302\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 117.6421 - accuracy: 0.53 - 0s 162us/sample - loss: 89.2557 - accuracy: 0.5521\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 73.2153 - accuracy: 0.593 - 0s 328us/sample - loss: 46.7921 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 66.2855 - accuracy: 0.625 - 0s 328us/sample - loss: 50.5706 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 126.9360 - accuracy: 0.56 - 0s 344us/sample - loss: 81.9624 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 30.6918 - accuracy: 0.812 - 0s 328us/sample - loss: 49.3000 - accuracy: 0.6875\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 10.3059 - accuracy: 0.843 - 0s 328us/sample - loss: 32.4161 - accuracy: 0.8125\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 108.7109 - accuracy: 0.40 - 0s 161us/sample - loss: 98.9490 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 67.4602 - accuracy: 0.625 - 0s 167us/sample - loss: 94.4259 - accuracy: 0.5729\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 75.1406 - accuracy: 0.593 - 0s 177us/sample - loss: 69.4358 - accuracy: 0.5833\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 55.2682 - accuracy: 0.687 - 0s 328us/sample - loss: 49.7202 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 24.9712 - accuracy: 0.687 - 0s 328us/sample - loss: 28.2923 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 63.4713 - accuracy: 0.781 - 0s 375us/sample - loss: 40.9427 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 23.0377 - accuracy: 0.781 - 0s 359us/sample - loss: 28.6210 - accuracy: 0.7812\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 55.1535 - accuracy: 0.687 - 0s 391us/sample - loss: 45.8804 - accuracy: 0.7812\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 171.3783 - accuracy: 0.43 - 0s 167us/sample - loss: 111.0770 - accuracy: 0.5156\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 83.2851 - accuracy: 0.562 - 0s 198us/sample - loss: 95.9230 - accuracy: 0.5625\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 197.2317 - accuracy: 0.43 - 0s 187us/sample - loss: 89.8921 - accuracy: 0.5833\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 48.9461 - accuracy: 0.625 - 0s 344us/sample - loss: 53.8641 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 48.3813 - accuracy: 0.843 - 0s 359us/sample - loss: 47.5743 - accuracy: 0.7500\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 28.7607 - accuracy: 0.875 - 0s 329us/sample - loss: 43.5686 - accuracy: 0.7969\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 34.7590 - accuracy: 0.656 - 0s 359us/sample - loss: 53.6099 - accuracy: 0.6250\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 41.0080 - accuracy: 0.750 - 0s 375us/sample - loss: 39.8352 - accuracy: 0.7500\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 79.0827 - accuracy: 0.593 - 0s 162us/sample - loss: 105.4636 - accuracy: 0.5260\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 75.3559 - accuracy: 0.625 - 0s 172us/sample - loss: 95.8198 - accuracy: 0.5990\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 134.4590 - accuracy: 0.53 - 0s 193us/sample - loss: 76.9500 - accuracy: 0.5521\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 33.0615 - accuracy: 0.625 - 0s 375us/sample - loss: 46.8874 - accuracy: 0.6094\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 33.9061 - accuracy: 0.687 - 0s 375us/sample - loss: 33.3054 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 36.8147 - accuracy: 0.656 - 0s 484us/sample - loss: 34.4206 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 35.5789 - accuracy: 0.781 - 0s 281us/sample - loss: 35.0497 - accuracy: 0.7031\n",
      "Train on 64 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 36.0258 - accuracy: 0.718 - 0s 344us/sample - loss: 30.2870 - accuracy: 0.7500\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 140.3617 - accuracy: 0.37 - 0s 182us/sample - loss: 88.0480 - accuracy: 0.5677\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 75.8947 - accuracy: 0.656 - 0s 193us/sample - loss: 94.4073 - accuracy: 0.5417\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 88.5632 - accuracy: 0.593 - 0s 177us/sample - loss: 74.2051 - accuracy: 0.5260\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 61.9871 - accuracy: 0.625 - 0s 312us/sample - loss: 49.5028 - accuracy: 0.6719\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 20.6693 - accuracy: 0.781 - 0s 328us/sample - loss: 24.0483 - accuracy: 0.7656\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 99.3546 - accuracy: 0.593 - 0s 344us/sample - loss: 76.0297 - accuracy: 0.6094\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 33.0797 - accuracy: 0.750 - 0s 359us/sample - loss: 34.9180 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 63.8851 - accuracy: 0.625 - 0s 359us/sample - loss: 50.6001 - accuracy: 0.7188\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 80.8879 - accuracy: 0.468 - 0s 188us/sample - loss: 73.4490 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 116.7455 - accuracy: 0.50 - 0s 172us/sample - loss: 85.1085 - accuracy: 0.5573\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 73.6393 - accuracy: 0.531 - 0s 167us/sample - loss: 87.1925 - accuracy: 0.5469\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 65.8794 - accuracy: 0.593 - 0s 359us/sample - loss: 54.1372 - accuracy: 0.6562\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 23.1048 - accuracy: 0.718 - 0s 328us/sample - loss: 24.5915 - accuracy: 0.7344\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 43.3837 - accuracy: 0.750 - 0s 391us/sample - loss: 50.3122 - accuracy: 0.6250\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 35.0156 - accuracy: 0.718 - 0s 359us/sample - loss: 42.6163 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 54.5552 - accuracy: 0.656 - 0s 297us/sample - loss: 60.1360 - accuracy: 0.7031\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 53.6964 - accuracy: 0.656 - 0s 172us/sample - loss: 65.1561 - accuracy: 0.5833\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 76.9936 - accuracy: 0.593 - 0s 172us/sample - loss: 85.0441 - accuracy: 0.5469\n",
      "Train on 192 samples\n",
      "192/192 [==============================] - ETA: 0s - loss: 55.4793 - accuracy: 0.593 - 0s 156us/sample - loss: 86.1154 - accuracy: 0.5365\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 54.5272 - accuracy: 0.718 - 0s 297us/sample - loss: 46.5826 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 64.9010 - accuracy: 0.593 - 0s 328us/sample - loss: 48.1262 - accuracy: 0.6406\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 39.9878 - accuracy: 0.718 - 0s 359us/sample - loss: 39.3596 - accuracy: 0.7188\n",
      "Train on 64 samples\n",
      "64/64 [==============================] - ETA: 0s - loss: 35.1265 - accuracy: 0.656 - 0s 344us/sample - loss: 53.0910 - accuracy: 0.6250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-596-67c3dbf3fe6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0ma_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    607\u001b[0m   \u001b[1;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m   \u001b[1;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     \"\"\"\n\u001b[0;32m   1210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1212\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3414\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3415\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3416\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3417\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3418\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   2693\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1852\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 1854\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   1855\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1856\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   2687\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   2688\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2689\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   2632\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2634\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2635\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2636\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m           optional_features=optional_features)\n\u001b[0;32m    233\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, options, args, kwargs, caller_fn_scope)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted_for_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mpermutation\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    277\u001b[0m       \u001b[1;31m# than reusing the same range Tensor. (presumably because of buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m       \u001b[1;31m# forwarding.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m       \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   1417\u001b[0m       \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"limit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1419\u001b[1;33m       \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"delta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1420\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m       \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    700\u001b[0m       \u001b[1;31m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m       \u001b[1;31m# strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[0;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[0;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[1;32m-> 1184\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    225\u001b[0m   \"\"\"\n\u001b[0;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 227\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    269\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[0;32m    270\u001b[0m              \"dtype\": dtype_value},\n\u001b[1;32m--> 271\u001b[1;33m       name=name).outputs[0]\n\u001b[0m\u001b[0;32m    272\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3429\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3431\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1773\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1774\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shepe\\python\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(epochs)):\n",
    "    true_flights = true_data.loc[np.random.choice(true_data.shape[0], \n",
    "                                            batch_size_true, \n",
    "                                            replace=False)].values\n",
    "    \n",
    "    for j in range(3):\n",
    "        random_ids = np.random.choice(ids_list, batch_size_noise, replace=False)\n",
    "        fake_batch_df = pd.DataFrame(random_ids, columns=['User_ID']).join(user_data, on = 'User_ID')\n",
    "        fake_batch = fake_batch_df.values\n",
    "\n",
    "        fake_flights = G.predict(fake_batch.astype('float32'))\n",
    "\n",
    "        x = np.concatenate((true_flights, fake_flights))\n",
    "        y = np.ones([batch_size_true + batch_size_noise, 1])\n",
    "        y[batch_size_true:] = 0\n",
    "        d_loss = D.fit(x.astype('float32'), y.astype('float32'))\n",
    "    \n",
    "    for j in range(5):\n",
    "        random_ids = np.random.choice(ids_list, batch_size_noise, replace=False)\n",
    "        fake_batch_df = pd.DataFrame(random_ids, columns=['User_ID']).join(user_data, on = 'User_ID')\n",
    "        fake_batch = fake_batch_df.values\n",
    "\n",
    "        y = np.ones([batch_size_noise, 1])\n",
    "        a_loss = AM.fit(fake_batch.astype('float32'), y.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>138023.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.899998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>210.949997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7568.087891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.076118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>107211.0</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.083332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.650002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44987.269531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.299461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>121571.0</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>165.462982</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.192389</td>\n",
       "      <td>203.425003</td>\n",
       "      <td>92.996330</td>\n",
       "      <td>243.066666</td>\n",
       "      <td>81.788689</td>\n",
       "      <td>3831.249023</td>\n",
       "      <td>...</td>\n",
       "      <td>235.791824</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107.514160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>96513.0</td>\n",
       "      <td>602.500000</td>\n",
       "      <td>67.175148</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>579.691650</td>\n",
       "      <td>55.755367</td>\n",
       "      <td>1390.541626</td>\n",
       "      <td>60.728687</td>\n",
       "      <td>5354.669434</td>\n",
       "      <td>...</td>\n",
       "      <td>562.793335</td>\n",
       "      <td>895.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199.094711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>125621.0</td>\n",
       "      <td>284.333344</td>\n",
       "      <td>23.094011</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>394.622223</td>\n",
       "      <td>151.140686</td>\n",
       "      <td>300.166656</td>\n",
       "      <td>191.709152</td>\n",
       "      <td>17103.035156</td>\n",
       "      <td>...</td>\n",
       "      <td>315.614288</td>\n",
       "      <td>895.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.595757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>85911.0</td>\n",
       "      <td>219.666672</td>\n",
       "      <td>62.500668</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.658328</td>\n",
       "      <td>332.927765</td>\n",
       "      <td>126.148186</td>\n",
       "      <td>418.277771</td>\n",
       "      <td>74.786232</td>\n",
       "      <td>2604.202881</td>\n",
       "      <td>...</td>\n",
       "      <td>151.872513</td>\n",
       "      <td>895.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>164.987610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>112658.0</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>129.903809</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.785939</td>\n",
       "      <td>699.127808</td>\n",
       "      <td>45.283504</td>\n",
       "      <td>493.011108</td>\n",
       "      <td>47.236874</td>\n",
       "      <td>516.029602</td>\n",
       "      <td>...</td>\n",
       "      <td>1200.334351</td>\n",
       "      <td>895.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.012207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>171731.0</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>205.666672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>217.466660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3890.547852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.315720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>115473.0</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>369.700012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>304.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2095.425781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.290680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>152320.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>190.666672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>169.016663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>800.805664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.818054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1           2         3         4           5   \\\n",
       "0   138023.0  105.000000    0.000000  1.000000  0.000000   43.899998   \n",
       "1   107211.0  347.000000    0.000000  8.000000  0.000000   45.083332   \n",
       "2   121571.0  369.000000  165.462982  9.500000  9.192389  203.425003   \n",
       "3    96513.0  602.500000   67.175148  4.000000  0.000000  579.691650   \n",
       "4   125621.0  284.333344   23.094011  6.000000  2.000000  394.622223   \n",
       "..       ...         ...         ...       ...       ...         ...   \n",
       "59   85911.0  219.666672   62.500668  7.333333  6.658328  332.927765   \n",
       "60  112658.0  392.000000  129.903809  4.666667  3.785939  699.127808   \n",
       "61  171731.0  249.000000    0.000000  8.000000  0.000000  205.666672   \n",
       "62  115473.0  226.000000    0.000000  4.000000  0.000000  369.700012   \n",
       "63  152320.0   80.000000    0.000000  1.000000  0.000000  190.666672   \n",
       "\n",
       "            6            7           8             9   ...           12  \\\n",
       "0     0.000000   210.949997    0.000000   7568.087891  ...     0.000000   \n",
       "1     0.000000    66.650002    0.000000  44987.269531  ...     0.000000   \n",
       "2    92.996330   243.066666   81.788689   3831.249023  ...   235.791824   \n",
       "3    55.755367  1390.541626   60.728687   5354.669434  ...   562.793335   \n",
       "4   151.140686   300.166656  191.709152  17103.035156  ...   315.614288   \n",
       "..         ...          ...         ...           ...  ...          ...   \n",
       "59  126.148186   418.277771   74.786232   2604.202881  ...   151.872513   \n",
       "60   45.283504   493.011108   47.236874    516.029602  ...  1200.334351   \n",
       "61    0.000000   217.466660    0.000000   3890.547852  ...     0.000000   \n",
       "62    0.000000   304.750000    0.000000   2095.425781  ...     0.000000   \n",
       "63    0.000000   169.016663    0.000000    800.805664  ...     0.000000   \n",
       "\n",
       "       13   14    15    16   17    18   19   20          21  \n",
       "0   895.0  2.0  57.0  71.0  5.0   0.0  1.0  1.0  121.076118  \n",
       "1   895.0  1.0  41.0  53.0  5.0  20.0  1.0  1.0   90.299461  \n",
       "2    76.0  2.0  38.0  33.0  5.0   0.0  1.0  1.0  107.514160  \n",
       "3   895.0  2.0  51.0  37.0  5.0   0.0  1.0  1.0  199.094711  \n",
       "4   895.0  1.0  35.0  33.0  5.0  20.0  1.0  1.0   96.595757  \n",
       "..    ...  ...   ...   ...  ...   ...  ...  ...         ...  \n",
       "59  895.0  2.0  49.0  11.0  5.0   0.0  1.0  1.0  164.987610  \n",
       "60  895.0  1.0  59.0  54.0  5.0   0.0  1.0  1.0  140.012207  \n",
       "61  895.0  0.0  59.0  52.0  5.0   0.0  1.0  1.0  102.315720  \n",
       "62  895.0  2.0  40.0  54.0  5.0   0.0  1.0  1.0  157.290680  \n",
       "63  895.0  1.0  55.0  26.0  5.0   0.0  4.0  1.0  140.818054  \n",
       "\n",
       "[64 rows x 22 columns]"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(fake_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>SMS_IN_CNT_M3_mean</th>\n",
       "      <th>SMS_IN_CNT_M3_std</th>\n",
       "      <th>TRIP_DURATION_mean</th>\n",
       "      <th>TRIP_DURATION_std</th>\n",
       "      <th>MOU_IN_REVENUE_M3_mean</th>\n",
       "      <th>MOU_IN_REVENUE_M3_std</th>\n",
       "      <th>MOU_OUT_REVENUE_M3_mean</th>\n",
       "      <th>MOU_OUT_REVENUE_M3_std</th>\n",
       "      <th>DOU_DURATION_M3_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>ARPU_M3_std</th>\n",
       "      <th>most_freq</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>REGION</th>\n",
       "      <th>DEVICE_TYPE</th>\n",
       "      <th>OS</th>\n",
       "      <th>SUBSAGE_MF_SEGMENT</th>\n",
       "      <th>USING_INTERNET</th>\n",
       "      <th>TRIP_MAIN_COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>80000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>532.116667</td>\n",
       "      <td>78.731334</td>\n",
       "      <td>466.699996</td>\n",
       "      <td>67.222821</td>\n",
       "      <td>5381.978539</td>\n",
       "      <td>...</td>\n",
       "      <td>995.831065</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>80000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>532.116667</td>\n",
       "      <td>78.731334</td>\n",
       "      <td>466.699996</td>\n",
       "      <td>67.222821</td>\n",
       "      <td>5381.978539</td>\n",
       "      <td>...</td>\n",
       "      <td>995.831065</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>80001</td>\n",
       "      <td>395.050000</td>\n",
       "      <td>143.792788</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>4.749238</td>\n",
       "      <td>250.322501</td>\n",
       "      <td>107.936626</td>\n",
       "      <td>301.601666</td>\n",
       "      <td>79.642260</td>\n",
       "      <td>1857.871891</td>\n",
       "      <td>...</td>\n",
       "      <td>2428.142838</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>80001</td>\n",
       "      <td>395.050000</td>\n",
       "      <td>143.792788</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>4.749238</td>\n",
       "      <td>250.322501</td>\n",
       "      <td>107.936626</td>\n",
       "      <td>301.601666</td>\n",
       "      <td>79.642260</td>\n",
       "      <td>1857.871891</td>\n",
       "      <td>...</td>\n",
       "      <td>2428.142838</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>80001</td>\n",
       "      <td>395.050000</td>\n",
       "      <td>143.792788</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>4.749238</td>\n",
       "      <td>250.322501</td>\n",
       "      <td>107.936626</td>\n",
       "      <td>301.601666</td>\n",
       "      <td>79.642260</td>\n",
       "      <td>1857.871891</td>\n",
       "      <td>...</td>\n",
       "      <td>2428.142838</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378462</td>\n",
       "      <td>123215</td>\n",
       "      <td>141.714286</td>\n",
       "      <td>179.504609</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>2.984085</td>\n",
       "      <td>50.095238</td>\n",
       "      <td>63.497327</td>\n",
       "      <td>53.183333</td>\n",
       "      <td>67.300009</td>\n",
       "      <td>3864.477401</td>\n",
       "      <td>...</td>\n",
       "      <td>2443.794135</td>\n",
       "      <td>895</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378463</td>\n",
       "      <td>177003</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.229473</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>6.658328</td>\n",
       "      <td>241.600000</td>\n",
       "      <td>209.231738</td>\n",
       "      <td>163.577776</td>\n",
       "      <td>141.662510</td>\n",
       "      <td>6933.653649</td>\n",
       "      <td>...</td>\n",
       "      <td>7040.220730</td>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378464</td>\n",
       "      <td>180451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378465</td>\n",
       "      <td>156886</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>18.230012</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>192.750000</td>\n",
       "      <td>163.797605</td>\n",
       "      <td>419.761113</td>\n",
       "      <td>214.269399</td>\n",
       "      <td>1033.710614</td>\n",
       "      <td>...</td>\n",
       "      <td>624.788358</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378466</td>\n",
       "      <td>180452</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>225.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>224.416668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1720.324223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378467 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_ID  SMS_IN_CNT_M3_mean  SMS_IN_CNT_M3_std  TRIP_DURATION_mean  \\\n",
       "0         80000          299.000000           1.154701            4.500000   \n",
       "1         80000          299.000000           1.154701            4.500000   \n",
       "2         80001          395.050000         143.792788            4.650000   \n",
       "3         80001          395.050000         143.792788            4.650000   \n",
       "4         80001          395.050000         143.792788            4.650000   \n",
       "...         ...                 ...                ...                 ...   \n",
       "378462   123215          141.714286         179.504609            3.285714   \n",
       "378463   177003           58.000000          50.229473            5.333333   \n",
       "378464   180451            0.000000           0.000000           13.000000   \n",
       "378465   156886           35.333333          18.230012           12.666667   \n",
       "378466   180452           70.000000           0.000000            0.000000   \n",
       "\n",
       "        TRIP_DURATION_std  MOU_IN_REVENUE_M3_mean  MOU_IN_REVENUE_M3_std  \\\n",
       "0                1.290994              532.116667              78.731334   \n",
       "1                1.290994              532.116667              78.731334   \n",
       "2                4.749238              250.322501             107.936626   \n",
       "3                4.749238              250.322501             107.936626   \n",
       "4                4.749238              250.322501             107.936626   \n",
       "...                   ...                     ...                    ...   \n",
       "378462           2.984085               50.095238              63.497327   \n",
       "378463           6.658328              241.600000             209.231738   \n",
       "378464           0.000000                0.000000               0.000000   \n",
       "378465           2.081666              192.750000             163.797605   \n",
       "378466           0.000000              225.050000               0.000000   \n",
       "\n",
       "        MOU_OUT_REVENUE_M3_mean  MOU_OUT_REVENUE_M3_std  DOU_DURATION_M3_mean  \\\n",
       "0                    466.699996               67.222821           5381.978539   \n",
       "1                    466.699996               67.222821           5381.978539   \n",
       "2                    301.601666               79.642260           1857.871891   \n",
       "3                    301.601666               79.642260           1857.871891   \n",
       "4                    301.601666               79.642260           1857.871891   \n",
       "...                         ...                     ...                   ...   \n",
       "378462                53.183333               67.300009           3864.477401   \n",
       "378463               163.577776              141.662510           6933.653649   \n",
       "378464                 0.000000                0.000000              0.000000   \n",
       "378465               419.761113              214.269399           1033.710614   \n",
       "378466               224.416668                0.000000           1720.324223   \n",
       "\n",
       "        ...  ARPU_M3_std  most_freq  GENDER  AGE  REGION  DEVICE_TYPE  OS  \\\n",
       "0       ...   995.831065        222       2   48      55            5   0   \n",
       "1       ...   995.831065        222       2   48      55            5   0   \n",
       "2       ...  2428.142838        222       2   60      55            5  20   \n",
       "3       ...  2428.142838        222       2   60      55            5  20   \n",
       "4       ...  2428.142838        222       2   60      55            5  20   \n",
       "...     ...          ...        ...     ...  ...     ...          ...  ..   \n",
       "378462  ...  2443.794135        895       2   36      33            9  21   \n",
       "378463  ...  7040.220730        895       0   33      70            9  21   \n",
       "378464  ...     0.000000        895       0   -1       0            9  21   \n",
       "378465  ...   624.788358         12       0   -1      33            5  20   \n",
       "378466  ...     0.000000        895       0   -1      35            5  20   \n",
       "\n",
       "        SUBSAGE_MF_SEGMENT  USING_INTERNET  TRIP_MAIN_COUNTRY  \n",
       "0                        1            True                222  \n",
       "1                        1            True                 85  \n",
       "2                        1            True                213  \n",
       "3                        1            True                 23  \n",
       "4                        1            True                202  \n",
       "...                    ...             ...                ...  \n",
       "378462                   1            True                 19  \n",
       "378463                   1            True                175  \n",
       "378464                   0           False                  1  \n",
       "378465                   4            True                 12  \n",
       "378466                   2            True                189  \n",
       "\n",
       "[378467 rows x 22 columns]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto = G.predict(pd.DataFrame(ids_list, columns=['User_ID']).join(user_data, on = 'User_ID').values.astype('float32'))[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.20306 ],\n",
       "       [40.140156],\n",
       "       [40.668377],\n",
       "       [40.842495],\n",
       "       [40.715103],\n",
       "       [40.131947],\n",
       "       [40.285076],\n",
       "       [40.405567],\n",
       "       [40.714012],\n",
       "       [40.302696],\n",
       "       [40.590664],\n",
       "       [40.976707],\n",
       "       [40.60391 ],\n",
       "       [40.603962],\n",
       "       [40.012596],\n",
       "       [40.378994]], dtype=float32)"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toto[toto.astype('int32').reshape(-1) == 40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
